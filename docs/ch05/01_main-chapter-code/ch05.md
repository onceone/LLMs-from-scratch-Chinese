

# 第5章: 无监督预训练


```python
from importlib.metadata import version

pkgs = ["matplotlib", 
        "numpy", 
        "tiktoken", 
        "torch",
        "tensorflow" # For OpenAI's pretrained weights
       ]
for p in pkgs:
    print(f"{p} version: {version(p)}")
```

    matplotlib version: 3.9.0
    numpy version: 1.26.4
    tiktoken version: 0.7.0
    torch version: 2.4.0
    tensorflow version: 2.16.1


- 在本章中，我们实现了训练循环和基础模型评估代码，以对 LLM 进行预训练。

- 在本章结束时，我们还将从 OpenAI 加载公开可用的预训练权重到我们的模型中。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/chapter-overview.webp" width=500px>

- 本章涵盖的主题如下：

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp" width=400px>

## 5.1 评估生成文本模型
(Evaluating generative text models)

- We start this section with a brief recap of initializing a GPT model using the code from the previous chapter
- Then, we discuss basic evaluation metrics for LLMs
- Lastly, in this section, we apply these evaluation metrics to a training and validation dataset

### 5.1.1 Using GPT to generate text

- We initialize a GPT model using the code from the previous chapter


```python
import torch
from previous_chapters import GPTModel

GPT_CONFIG_124M = {
    "vocab_size": 50257,   # Vocabulary size
    "context_length": 256, # Shortened context length (orig: 1024)
    "emb_dim": 768,        # Embedding dimension
    "n_heads": 12,         # Number of attention heads
    "n_layers": 12,        # Number of layers
    "drop_rate": 0.1,      # Dropout rate
    "qkv_bias": False      # Query-key-value bias
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval();  # Disable dropout during inference
```

- 我们在上面使用了 0.1 的 dropout，但现在在训练大语言模型（LLM）时，通常会不使用 dropout。
- 现代 LLM 也不再在 `nn.Linear` 层的查询（query）、键（key）和值（value）矩阵中使用偏置向量（与早期的 GPT 模型不同），这通过设置 `"qkv_bias": False` 来实现。
- 我们将 `context_length` 限制为 256 个 token，以减少训练模型时的计算资源需求，而原始的 1.24 亿参数的 GPT-2 模型使用了 1024 个 token。
  - 这样做是为了让更多的读者能够在自己的笔记本电脑上跟随并执行代码示例。
  - 然而，请随时将 `context_length` 增加到 1024 个 token（这不需要修改任何代码）。
  - 我们稍后会从预训练的权重中加载一个 `context_length` 为 1024 的模型。

- 接下来，我们使用上一章的 `generate_text_simple` 函数来生成文本。
- 此外，我们还定义了两个便捷函数，`text_to_token_ids` 和 `token_ids_to_text`，用于在 token 和文本表示之间进行转换，这些函数将在本章中多次使用。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-process.webp" width=500px>


```python
import tiktoken
from previous_chapters import generate_text_simple

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension
    return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0) # remove batch dimension
    return tokenizer.decode(flat.tolist())

start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_context, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"]
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

    Output text:
     Every effort moves you rentingetic wasnم refres RexMeCHicular stren


- 如上所示，由于模型尚未训练，生成的文本质量较差。
- 那么，我们如何用数字化的方式衡量“好文本”是什么，以便在训练过程中跟踪其进展呢？
- 接下来的小节介绍了用于计算生成输出损失的评估指标，能够帮助我们衡量训练进展。
- 后续章节中关于微调 LLM 的内容，还将介绍更多衡量模型质量的方式。

<br>

### 5.1.2 计算文本生成损失：交叉熵与困惑度
(Calculating the text generation loss: cross-entropy and perplexity)

- 假设我们有一个 `inputs` 张量，其中包含两个训练示例（行）的 token ID。
- 对应于 `inputs`，`targets` 张量包含模型应该生成的目标 token ID。
- 注意到 `targets` 是通过将 `inputs` 向右平移一个位置得到的，这一点在第 2 章实现数据加载器时有解释。  
  - 例如，假设某个输入序列是 `[1, 2, 3, 4]`，那么相应的目标序列应该是 `[2, 3, 4, 5]`。
  - 这种方式确保了模型在训练时是预测下一个 token（而不是预测整个序列）。


```python
inputs = torch.tensor([[16833, 3626, 6100],   # ["every effort moves",
                       [40,    1107, 588]])   #  "I really like"]

targets = torch.tensor([[3626, 6100, 345  ],  # [" effort moves you",
                        [1107,  588, 11311]]) #  " really like chocolate"]
```

- 将 `inputs` 输入到模型中，我们获得了两个包含 3 个 token 的输入示例的 logits 向量。
- 每个 token 是一个 50,257 维的向量，对应于词汇表的大小。
- 通过应用 softmax 函数，我们可以将 logits 张量转换为一个相同维度的张量，其中包含概率分数。


```python
with torch.no_grad():
    logits = model(inputs)

probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary
print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)
```

    torch.Size([2, 3, 50257])


- 下图使用了一个非常小的词汇表作为示例，展示了如何将概率分数转换回文本，这个过程我们在上一章的最后讨论过。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp" width=500px>

- 如上一章所述，我们可以应用 `argmax` 函数将概率分数转换为预测的 token ID。
- 上面的 softmax 函数为每个 token 生成了一个 50,257 维的向量；`argmax` 函数返回该向量中最高概率分数的位置，这个位置即为给定 token 的预测 token ID。

- 由于我们有 2 个输入批次，每个批次包含 3 个 token，因此我们会得到一个 2x3 的预测 token ID 矩阵：


```python
token_ids = torch.argmax(probas, dim=-1, keepdim=True)
print("Token IDs:\n", token_ids)
```

    Token IDs:
     tensor([[[16657],
             [  339],
             [42826]],
    
            [[49906],
             [29669],
             [41751]]])


- 如果我们解码这些 tokens，会发现它们与我们希望模型预测的目标 tokens 相差甚远：


```python
print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
print(f"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")
```

    Targets batch 1:  effort moves you
    Outputs batch 1:  Armed heNetflix


- 这是因为模型还没有经过训练。
- 要训练模型，我们需要知道模型的预测与正确预测（目标）之间的差距。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp" width=500px>

- 与目标索引对应的令牌概率如下所示：


```python
text_idx = 0
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 1:", target_probas_1)

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 2:", target_probas_2)
```

    Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
    Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])


- 我们希望最大化所有这些值，使它们接近1的概率值。
- 在数学优化中，最大化概率分数的对数比直接最大化概率分数更为容易；虽然这超出了本书的范围，但我已经录制了一节讲座，详细介绍了更多内容：[L8.2 Logistic Regression Loss Function](https://www.youtube.com/watch?v=GxJe0DZvydM)。


```python
# Compute logarithm of all token probabilities
log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(log_probas)
```

    tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])


- 接下来，我们计算平均对数概率：


```python
# Calculate the average probability for each token
avg_log_probas = torch.mean(log_probas)
print(avg_log_probas)
```

    tensor(-10.7940)


- 目标是通过优化模型权重，使得这个平均对数概率尽可能大。
- 由于使用了对数，最大的可能值是 0，而我们当前距离 0 还很远。

- 在深度学习中，通常的做法是最小化*负*的平均对数概率值，而不是最大化平均对数概率。在我们的例子中，深度学习中我们不会最大化 -10.7722 使其接近 0，而是最小化 10.7722，使其接近 0。
- 这个负的平均对数概率值，也就是 10.7722，在深度学习中被称为交叉熵损失（cross-entropy loss）。


```python
neg_avg_log_probas = avg_log_probas * -1
print(neg_avg_log_probas)
```

    tensor(10.7940)


- PyTorch 已经实现了一个 `cross_entropy` 函数，用于执行之前的步骤。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp?123" width=400px>

- 在应用 `cross_entropy` 函数之前，我们先检查一下 logits 和 targets 的形状。


```python
# Logits have shape (batch_size, num_tokens, vocab_size)
print("Logits shape:", logits.shape)

# Targets have shape (batch_size, num_tokens)
print("Targets shape:", targets.shape)
```

    Logits shape: torch.Size([2, 3, 50257])
    Targets shape: torch.Size([2, 3])


- 对于 PyTorch 中的 `cross_entropy` 函数，我们希望通过将张量在批次维度上展开来扁平化它们：


```python
logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()

print("Flattened logits:", logits_flat.shape)
print("Flattened targets:", targets_flat.shape)
```

    Flattened logits: torch.Size([6, 50257])
    Flattened targets: torch.Size([6])


- 请注意，目标是令牌 ID，这些 ID 也表示我们希望在 logits 张量中最大化的索引位置。
- PyTorch 中的 `cross_entropy` 函数将自动处理软最大化（softmax）和对数概率计算，它会在 logits 中对应该最大化的令牌索引进行处理。


```python
loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(loss)
```

    tensor(10.7940)


- 与交叉熵损失相关的一个概念是 LLM 的困惑度（perplexity）。
- 困惑度实际上是交叉熵损失的指数。


```python
perplexity = torch.exp(loss)
print(perplexity)
```

    tensor(48725.8203)


- 困惑度通常被认为更具可解释性，因为它可以理解为模型在每一步上不确定的有效词汇大小（在上面的例子中，即为 48,725 个词或标记）。
- 换句话说，困惑度提供了一个衡量模型预测的概率分布与数据集中实际词汇分布匹配程度的指标。
- 与损失函数类似，较低的困惑度表示模型预测与实际分布的差距更小。

### 5.1.3 计算训练集和验证集的损失

我们使用相对较小的数据集来训练大型语言模型（LLM）（实际上，仅仅使用了一个短篇故事）。

原因如下：
- 这样做的好处是，你可以在没有适当 GPU 的笔记本电脑上在几分钟内运行这些代码示例。
- 训练过程相对较快（几分钟而不是几周），这对于教学目的来说非常合适。
- 我们使用的是公共领域的文本，可以合法地包含在这个 GitHub 仓库中，而不会违反任何使用权或膨胀仓库的大小。

例如，Llama 2 7B 模型在 A100 GPU 上训练了 2 万亿个 token，耗时 184,320 GPU 小时。
- 在撰写本文时，AWS 上 8xA100 云服务器的每小时成本大约是 30 美元。
- 因此，通过一个大致估算，训练这个 LLM 的费用为 184,320 / 8 * 30 = 690,000 美元。

下面，我们将使用第 2 章中使用的相同数据集进行训练。


```python
import os
import urllib.request

file_path = "the-verdict.txt"
url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

if not os.path.exists(file_path):
    with urllib.request.urlopen(url) as response:
        text_data = response.read().decode('utf-8')
    with open(file_path, "w", encoding="utf-8") as file:
        file.write(text_data)
else:
    with open(file_path, "r", encoding="utf-8") as file:
        text_data = file.read()
```

- 为了快速检查文本是否成功加载，我们可以打印出前后各 100 个单词。


```python
# First 100 characters
print(text_data[:99])
```

    I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no 



```python
# Last 100 characters
print(text_data[-99:])
```

    it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art."



```python
total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))

print("Characters:", total_characters)
print("Tokens:", total_tokens)
```

    Characters: 20479
    Tokens: 5145


- 这段文本有 5,145 个 token，虽然对于训练一个 LLM 来说非常短，但这仅仅是为了教学目的（我们稍后也会加载预训练的权重）。

- 接下来，我们将数据集划分为训练集和验证集，并使用第2章中的数据加载器来准备 LLM 训练的批次。
- 为了可视化的目的，下面的图假设 `max_length=6`，但对于训练加载器，我们将 `max_length` 设置为 LLM 所支持的上下文长度。图中仅显示输入的 tokens 以简化示例。

- 由于我们训练 LLM 预测文本中的下一个单词，目标和输入基本相同，唯一的区别是目标会向右移动一个位置。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp" width=500px>


```python
from previous_chapters import create_dataloader_v1

# Train/validation ratio
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]


torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0
)

val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0
)
```


```python
# Sanity check

if total_tokens * (train_ratio) < GPT_CONFIG_124M["context_length"]:
    print("Not enough tokens for the training loader. "
          "Try to lower the `GPT_CONFIG_124M['context_length']` or "
          "increase the `training_ratio`")

if total_tokens * (1-train_ratio) < GPT_CONFIG_124M["context_length"]:
    print("Not enough tokens for the validation loader. "
          "Try to lower the `GPT_CONFIG_124M['context_length']` or "
          "decrease the `training_ratio`")
```

- 我们使用相对较小的批次大小来减少计算资源的需求，因为数据集本身非常小。
- 例如，Llama 2 7B 是使用 1024 的批次大小进行训练的。

- 一个可选的检查，确保数据已正确加载：


```python
print("Train loader:")
for x, y in train_loader:
    print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
    print(x.shape, y.shape)
```

    Train loader:
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    torch.Size([2, 256]) torch.Size([2, 256])
    
    Validation loader:
    torch.Size([2, 256]) torch.Size([2, 256])


- 另一个可选检查，确保令牌大小在预期的范围内：


```python
train_tokens = 0
for input_batch, target_batch in train_loader:
    train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
    val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)
```

    Training tokens: 4608
    Validation tokens: 512
    All tokens: 5120


- 接下来，我们实现一个工具函数，用于计算给定批次的交叉熵损失。
- 此外，我们还实现第二个工具函数，用于计算数据加载器中用户指定数量批次的损失。


```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
```

- 如果你使用的是支持CUDA的GPU，代码中无需任何更改，LLM将自动在GPU上进行训练。
- 通过设置 `device`，我们确保数据与LLM模型加载到同一个设备上。


```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Note:
# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,
# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).
# However, the resulting loss values may be slightly different.

#if torch.cuda.is_available():
#    device = torch.device("cuda")
#elif torch.backends.mps.is_available():
#    device = torch.device("mps")
#else:
#    device = torch.device("cpu")
#
# print(f"Using {device} device.")


model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes


torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader

with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
    train_loss = calc_loss_loader(train_loader, model, device)
    val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```

    Training loss: 10.987583584255642
    Validation loss: 10.98110580444336


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp" width=400px>

## 5.2 训练大型语言模型
(Training an LLM)

- 在本节中，我们最终实现了训练大型语言模型（LLM）的代码
- 我们将重点关注一个简单的训练函数（如果你有兴趣将此训练函数扩展为更多高级技术，如学习率预热、余弦退火和梯度裁剪，请参考[附录D](../../appendix-D/01_main-chapter-code)）

![训练步骤](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp)


```python
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode
        
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad() # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward() # Calculate loss gradients
            optimizer.step() # Update model weights using loss gradients
            tokens_seen += input_batch.numel()
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # Print a sample text after each epoch
        generate_and_print_sample(
            model, tokenizer, device, start_context
        )

    return train_losses, val_losses, track_tokens_seen


def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss


def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
    decoded_text = token_ids_to_text(token_ids, tokenizer)
    print(decoded_text.replace("\n", " "))  # Compact print format
    model.train()
```

- 现在，让我们使用上面定义的训练函数来训练大型语言模型（LLM）：


```python
# Note:
# Uncomment the following code to calculate the execution time
# import time
# start_time = time.time()

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context="Every effort moves you", tokenizer=tokenizer
)

# Note:
# Uncomment the following code to show the execution time
# end_time = time.time()
# execution_time_minutes = (end_time - start_time) / 60
# print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```

    Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933
    Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339
    Every effort moves you,,,,,,,,,,,,.                                     
    Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048
    Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616
    Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,
    Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600
    Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348
    Every effort moves you, and I had been.                                            
    Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278
    Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226
    Every effort moves you know the                          "I he had the donkey and I had the and I had the donkey and down the room, I had
    Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160
    Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            "Oh, and he said, and down the room, and in
    Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179
    Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141
    Every effort moves you know," was one of the picture. The--I had a little of a little: "Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had
    Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134
    Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233
    Every effort moves you know," was one of the picture for nothing--I told Mrs.  "I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his
    Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238
    Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242
    Every effort moves you know," was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. "strongest," as his
    Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293
    Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393
    Every effort moves you?"  "Yes--quite insensible to the irony. She wanted him vindicated--and by me!"  He laughed again, and threw back the window-curtains, I had the donkey. "There were days when I
    Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452
    Every effort moves you know," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis



```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator


def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    plt.savefig("loss-plot.pdf")
    plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```


    
![png](output_79_0.png)
    


- 从上面的结果来看，我们可以看到，模型一开始生成的字符串是无法理解的，但到了后期，它能够生成语法上基本正确的句子。
- 然而，从训练集和验证集的损失值来看，我们可以看到模型开始出现过拟合的现象。
- 如果我们查看它在训练后期生成的几段文本，我们会发现它们与训练集中的内容完全一致——模型只是简单地记住了训练数据。
- 后续我们将介绍一些解码策略，可以在一定程度上缓解这种记忆现象。
- 需要注意的是，过拟合的原因在于我们使用了一个非常非常小的训练集，并且对其进行了多次迭代。
  - 这里的 LLM 训练主要用于教育目的；我们主要希望看到模型能够学习生成连贯的文本。
  - 与其花费数周或数月的时间，在昂贵的硬件上训练这个模型，不如后续直接加载预训练的权重。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp" width=350px>

**如果你有兴趣在训练函数中加入更高级的技术，比如学习率预热（learning rate warmup）、余弦退火（cosine annealing）和梯度裁剪（gradient clipping），请参考 [附录 D](../../appendix-D/01_main-chapter-code)。**

**如果你有兴趣使用更大的训练数据集和更长时间的训练，请参阅 [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)。**

## 5.3 解码策略以控制随机性
(Decoding strategies to control randomness)

- 对于我们上面训练的相对较小的LLM（如GPT模型），推理的计算开销相对较小，因此即使在训练时使用了GPU，进行推理时也无需使用GPU。
- 使用我们之前在简单训练函数中用到的 `generate_text_simple` 函数（参见上一章），我们可以一次生成一个新的词（或标记）。
- 正如在第5.1.2节中所解释的，下一步生成的标记是对应于词汇表中概率得分最大的标记。


```python
model.to("cpu")
model.eval()

tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=25,
    context_size=GPT_CONFIG_124M["context_length"]
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

    Output text:
     Every effort moves you know," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun


- 即使我们多次执行上述的 `generate_text_simple` 函数，LLM 仍然会生成相同的输出。
- 现在我们引入两个概念，即所谓的解码策略，来修改 `generate_text_simple`：*温度缩放*（temperature scaling）和 *top-k 采样*（top-k sampling）。
- 这些策略将允许模型控制生成文本的随机性和多样性。

### 5.3.1 温度缩放
(Temperature scaling)

- 之前，我们总是使用 `torch.argmax` 选择具有最高概率的 token 作为下一个 token。
- 为了增加多样性，我们可以使用 `torch.multinomial(probs, num_samples=1)` 从概率分布中采样下一个 token。
- 在这里，每个索引被选中的概率与输入张量中该位置的概率相对应。

- 这里是生成下一个 token 的简要回顾，假设使用一个非常小的词汇表来做说明：


```python
vocab = { 
    "closer": 0,
    "every": 1, 
    "effort": 2, 
    "forward": 3,
    "inches": 4,
    "moves": 5, 
    "pizza": 6,
    "toward": 7,
    "you": 8,
} 

inverse_vocab = {v: k for k, v in vocab.items()}

# Suppose input is "every effort moves you", and the LLM
# returns the following logits for the next token:
next_token_logits = torch.tensor(
    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]
)

probas = torch.softmax(next_token_logits, dim=0)
next_token_id = torch.argmax(probas).item()

# The next generated token is then as follows:
print(inverse_vocab[next_token_id])
```

    forward



```python
torch.manual_seed(123)
next_token_id = torch.multinomial(probas, num_samples=1).item()
print(inverse_vocab[next_token_id])
```

    forward


- 我们不再通过 `torch.argmax` 来确定最可能的 token，而是使用 `torch.multinomial(probas, num_samples=1)` 从 softmax 概率分布中进行采样，确定最可能的 token。
- 为了说明这一点，我们来看一下，当我们使用原始的 softmax 概率进行 1,000 次采样时会发生什么：


```python
def print_sampled_tokens(probas):
    torch.manual_seed(123) # Manual seed for reproducibility
    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]
    sampled_ids = torch.bincount(torch.tensor(sample))
    for i, freq in enumerate(sampled_ids):
        print(f"{freq} x {inverse_vocab[i]}")

print_sampled_tokens(probas)
```

    73 x closer
    0 x every
    0 x effort
    582 x forward
    2 x inches
    0 x moves
    0 x pizza
    343 x toward


- 我们可以通过一个叫做“温度缩放”的概念来控制分布和选择过程。
- “温度缩放”其实就是用一个大于 0 的数字来除 logits。
- 温度大于 1 时，应用 softmax 后的 token 概率会更加均匀分布。
- 温度小于 1 时，应用 softmax 后的分布会更加自信（即更尖锐或更集中的分布）。


```python
def softmax_with_temperature(logits, temperature):
    scaled_logits = logits / temperature
    return torch.softmax(scaled_logits, dim=0)

# Temperature values
temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence

# Calculate scaled probabilities
scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]
```


```python
# Plotting
x = torch.arange(len(vocab))
bar_width = 0.15

fig, ax = plt.subplots(figsize=(5, 3))
for i, T in enumerate(temperatures):
    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')

ax.set_ylabel('Probability')
ax.set_xticks(x)
ax.set_xticklabels(vocab.keys(), rotation=90)
ax.legend()

plt.tight_layout()
plt.savefig("temperature-plot.pdf")
plt.show()
```


    
![png](output_97_0.png)
    


- 我们可以看到，通过温度 0.1 进行的重新缩放会导致分布更加尖锐，接近 `torch.argmax`，使得最可能的单词几乎总是被选择。


```python
print_sampled_tokens(scaled_probas[1])
```

    0 x closer
    0 x every
    0 x effort
    985 x forward
    0 x inches
    0 x moves
    0 x pizza
    15 x toward


- 通过温度 5 进行重新缩放后的概率分布更加均匀：


```python
print_sampled_tokens(scaled_probas[2])
```

    165 x closer
    75 x every
    42 x effort
    239 x forward
    71 x inches
    46 x moves
    32 x pizza
    227 x toward
    103 x you


- 假设LLM的输入是“every effort moves you”，使用上述方法有时会导致生成无意义的文本，比如“every effort moves you pizza”，这种情况会发生 3.2% 的时间（1000次中有32次）。

### 5.3.2 Top-k sampling

- 为了能够使用更高的温度来增加输出的多样性，并减少生成无意义句子的概率，我们可以将采样的 tokens 限制为 top-k 最可能的 tokens。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp" width=500px>

- （请注意，图中的数字被截断为小数点后两位，以减少视觉干扰。Softmax 行中的值应加起来为 1.0。）

- 在代码中，我们可以这样实现：


```python
top_k = 3
top_logits, top_pos = torch.topk(next_token_logits, top_k)

print("Top logits:", top_logits)
print("Top positions:", top_pos)
```

    Top logits: tensor([6.7500, 6.2800, 4.5100])
    Top positions: tensor([3, 7, 0])



```python
new_logits = torch.where(
    condition=next_token_logits < top_logits[-1],
    input=torch.tensor(float("-inf")), 
    other=next_token_logits
)

print(new_logits)
```

    tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])


> 注意：
>
> 之前代码单元的一个替代方案，更高效的实现方式如下：
> 
> ```python
> new_logits = torch.full_like( # create tensor containing -inf values
>    next_token_logits, -torch.inf
>)   
> new_logits[top_pos] = next_token_logits[top_pos] # copy top k values into the -inf tensor
> ```
> <br>
> 更多细节请参见 [https://github.com/rasbt/LLMs-from-scratch/discussions/326](https://github.com/rasbt/LLMs-from-scratch/discussions/326)



```python
topk_probas = torch.softmax(new_logits, dim=0)
print(topk_probas)
```

    tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])


### 5.3.3 修改文本生成函数
(Modifying the text generation function)

- 前两个小节介绍了温度采样和top-k采样。
- 让我们使用这两个概念来修改我们之前用来通过LLM生成文本的`generate_simple`函数，创建一个新的`generate`函数：


```python
def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float("-inf")).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified
            break

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)

    return idx
```


```python
torch.manual_seed(123)

token_ids = generate(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=15,
    context_size=GPT_CONFIG_124M["context_length"],
    top_k=25,
    temperature=1.4
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

    Output text:
     Every effort moves you stand to work on surprise, a one of us had gone with random-


## 5.4 加载和保存模型权重
(Loading and saving model weights in PyTorch)

- 训练LLM计算成本很高，因此能够保存和加载LLM权重非常重要。
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp" width=400px>

- 在PyTorch中，推荐的方式是通过将`.state_dict()`方法应用于`torch.save`函数来保存模型的权重，即所谓的`state_dict`：


```python
torch.save(model.state_dict(), "model.pth")
```

- 然后，我们可以通过如下方式将模型权重加载到一个新的 `GPTModel` 实例中：


```python
model = GPTModel(GPT_CONFIG_124M)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.load_state_dict(torch.load("model.pth", map_location=device, weights_only=True))
model.eval();
```

- 通常，训练 LLM 时会使用像 Adam 或 AdamW 这样的自适应优化器，而不是常规的 SGD。
- 这些自适应优化器会为每个模型权重存储额外的参数，因此，如果我们计划稍后继续进行预训练，那么保存这些优化器的状态也是有意义的：


```python
torch.save({
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    }, 
    "model_and_optimizer.pth"
)
```


```python
checkpoint = torch.load("model_and_optimizer.pth", weights_only=True)

model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])

optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train();
```

## 5.5 从OpenAI加载预训练权重
(Loading pretrained weights from OpenAI)

- 之前，我们只使用一个非常小的短篇小说书籍训练了一个小型的 GPT-2 模型，目的是为了教学目的。
- 感兴趣的读者还可以在 [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg) 中找到使用完整的 Project Gutenberg 图书语料库进行的更长时间的预训练。
- 幸运的是，我们不必花费数十万到数十万美元去在一个大规模预训练语料库上预训练模型，而是可以加载 OpenAI 提供的预训练权重。

- 有关从 Hugging Face Hub 加载权重的替代方法，请参见 [../02_alternative_weight_loading](../02_alternative_weight_loading)。

- 首先，编写一些样板代码来从 OpenAI 下载文件并将权重加载到 Python 中
- 由于 OpenAI 使用了 [TensorFlow](https://www.tensorflow.org/)，因此我们需要安装并使用 TensorFlow 来加载权重；[tqdm](https://github.com/tqdm/tqdm) 是一个进度条库
- 取消注释并运行下一个代码单元来安装所需的库


```python
# pip install tensorflow tqdm
```


```python
print("TensorFlow version:", version("tensorflow"))
print("tqdm version:", version("tqdm"))
```

    TensorFlow version: 2.16.1
    tqdm version: 4.66.4



```python
# Relative import from the gpt_download.py contained in this folder
from gpt_download import download_and_load_gpt2
```

- 我们可以按如下代码下载 124M 参数模型的模型权重：


```python
settings, params = download_and_load_gpt2(model_size="124M", models_dir="gpt2")
```

    File already exists and is up-to-date: gpt2/124M/checkpoint
    File already exists and is up-to-date: gpt2/124M/encoder.json
    File already exists and is up-to-date: gpt2/124M/hparams.json
    File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001
    File already exists and is up-to-date: gpt2/124M/model.ckpt.index
    File already exists and is up-to-date: gpt2/124M/model.ckpt.meta
    File already exists and is up-to-date: gpt2/124M/vocab.bpe



```python
print("Settings:", settings)
```

    Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}



```python
print("Parameter dictionary keys:", params.keys())
```

    Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])



```python
print(params["wte"])
print("Token embedding weight tensor dimensions:", params["wte"].shape)
```

    [[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208
       0.04531523]
     [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983
       0.04318958]
     [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379
      -0.08785918]
     ...
     [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269
      -0.06952604]
     [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701
      -0.02245961]
     [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823
       0.12067825]]
    Token embedding weight tensor dimensions: (50257, 768)


- 可选地，"355M"、"774M" 和 "1558M" 也支持 `model_size` 参数
- 这些不同大小的模型之间的差异总结在下图所示：

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123" width=500px>

- 如上，我们将 124M GPT-2 模型权重加载到 Python 中，但是仍然需要将它们转移到我们的 `GPTModel` 实例中
- 首先，我们初始化一个新的 GPTModel 实例
- 请注意，原始的 GPT 模型在多头注意力模块中对查询、键和值矩阵的线性层进行了偏置向量的初始化，这是不必要或不推荐的；然而，为了能够正确加载权重，我们也必须通过将 `qkv_bias` 设置为 `True` 来启用这些
- 我们还在使用原始 GPT-2 模型使用的 `1024` 令牌上下文长度


```python
# Define model configurations in a dictionary for compactness
model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

# Copy the base configuration and update with specific model settings
model_name = "gpt2-small (124M)"  # Example model name
NEW_CONFIG = GPT_CONFIG_124M.copy()
NEW_CONFIG.update(model_configs[model_name])
NEW_CONFIG.update({"context_length": 1024, "qkv_bias": True})

gpt = GPTModel(NEW_CONFIG)
gpt.eval();
```

- 下一个任务是将 OpenAI 权重分配给 `GPTModel` 实例中相应的权重张量


```python
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))
```


```python
import numpy as np

def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])
    
    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight, 
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias, 
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight, 
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias, 
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight, 
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias, 
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale, 
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift, 
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale, 
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift, 
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])
    
    
load_weights_into_gpt(gpt, params)
gpt.to(device);
```

- 如果模型正确加载，我们可以使用我们之前的`generate`函数生成新的文本：


```python
torch.manual_seed(123)

token_ids = generate(
    model=gpt,
    idx=text_to_token_ids("Every effort moves you", tokenizer).to(device),
    max_new_tokens=25,
    context_size=NEW_CONFIG["context_length"],
    top_k=50,
    temperature=1.5
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

    Output text:
     Every effort moves you toward finding an ideal new way to practice something!
    
    What makes us want to be on top of that?
    
    


- 我们知道我们正确加载了模型权重，因为模型可以生成连贯的文本；如果我们犯了一个小错误，模型将无法做到这一点

- 如果你有兴趣了解如何从 Hugging Face Hub 加载权重，请参见 [../02_alternative_weight_loading](../02_alternative_weight_loading)
- 如果你想了解 GPT 架构与 Llama 架构（由 Meta AI 开发的流行 LLM）之间的比较，请参见额外内容 [../07_gpt_to_llama](../07_gpt_to_llama)

## 总结

- 请参阅 [./gpt_train.py](./gpt_train.py) 脚本，这是一个独立的训练脚本
- [./gpt_generate.py](./gpt_generate.py) 脚本加载来自 OpenAI 的预训练权重，并基于输入提示生成文本
- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 找到练习的解答
