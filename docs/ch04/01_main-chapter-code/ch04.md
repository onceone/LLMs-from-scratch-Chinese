

# 第4章: 从头实现 GPT 模型以生成文本
(Implementing a GPT model from Scratch To Generate Text )


```python
from importlib.metadata import version

import matplotlib
import tiktoken
import torch

print("matplotlib version:", version("matplotlib"))
print("torch version:", version("torch"))
print("tiktoken version:", version("tiktoken"))
```

    matplotlib version: 3.9.0
    torch version: 2.4.0
    tiktoken version: 0.7.0


- 在本章中，我们实现了一个类似 GPT 的大语言模型（LLM）架构；下一章将专注于训练这个 LLM。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp" width="500px">

## 4.1 编码一个大语言模型（LLM）架构
(Coding an LLM architecture)

- 第一章讨论了像 GPT 和 Llama 这样的模型，这些模型按顺序生成单词，并且基于原始 Transformer 架构的解码器部分。
- 因此，这些大语言模型通常被称为“解码器式”大语言模型。
- 与传统的深度学习模型相比，大语言模型通常更大，主要是由于它们巨大的参数量，而不是代码的多少。
- 我们将看到，大语言模型的架构中有许多元素是重复的。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp" width="400px">

- 在前几章中，我们使用了较小的嵌入维度来简化令牌输入和输出，以便于说明，确保它们适合在一页纸上显示。
- 在本章中，我们考虑类似于小型 GPT-2 模型的嵌入和模型大小。
- 我们将特别编写最小 GPT-2 模型的架构（1.24 亿个参数），如 Radford 等人的 [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 中所述（请注意，初始报告将其列为 1.17 亿参数，但后来在模型权重库中进行了更正）。
- 第 6 章将展示如何将预训练的权重加载到我们的实现中，这将与 3.45 亿、7.62 亿和 15.42 亿参数的模型大小兼容。

- 124 亿参数的 GPT-2 模型的配置细节包括：


```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,    # Vocabulary size
    "context_length": 1024, # Context length
    "emb_dim": 768,         # Embedding dimension
    "n_heads": 12,          # Number of attention heads
    "n_layers": 12,         # Number of layers
    "drop_rate": 0.1,       # Dropout rate
    "qkv_bias": False       # Query-Key-Value bias
}
```

- 我们使用简短的变量名，以避免后续代码行过长  
- `"vocab_size"` 表示词汇表大小为 50,257 个词，这由第 2 章讨论的 BPE 分词器支持  
- `"context_length"` 表示模型的最大输入标记数，这是由第 2 章讨论的位置嵌入所支持的  
- `"emb_dim"` 是令牌输入的嵌入大小，将每个输入令牌转换为一个 768 维的向量  
- `"n_heads"` 是多头注意力机制中的注意力头数量，参见第 3 章的实现  
- `"n_layers"` 是模型中 transformer 块的数量，我们将在后续章节中实现  
- `"drop_rate"` 是 dropout 机制的强度，参见第 3 章；0.1 意味着在训练过程中丢弃 10% 的隐藏单元，以缓解过拟合  
- `"qkv_bias"` 决定多头注意力机制中的 `Linear` 层在计算查询（Q）、键（K）和值（V）张量时，是否包含偏置向量；我们将禁用此选项，这是现代 LLM 的标准做法；不过，在第 5 章加载 OpenAI 预训练 GPT-2 权重到我们的重实现时，我们会再次回顾这一点

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp" width="500px">


```python
import torch
import torch.nn as nn


class DummyGPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        
        # Use a placeholder for TransformerBlock
        self.trf_blocks = nn.Sequential(
            *[DummyTransformerBlock(cfg) for _ in range(cfg["n_layers"])])
        
        # Use a placeholder for LayerNorm
        self.final_norm = DummyLayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(
            cfg["emb_dim"], cfg["vocab_size"], bias=False
        )

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits


class DummyTransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        # A simple placeholder

    def forward(self, x):
        # This block does nothing and just returns its input.
        return x


class DummyLayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5):
        super().__init__()
        # The parameters here are just to mimic the LayerNorm interface.

    def forward(self, x):
        # This layer does nothing and just returns its input.
        return x
```

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp?123" width="500px">


```python
import tiktoken

tokenizer = tiktoken.get_encoding("gpt2")

batch = []

txt1 = "Every effort moves you"
txt2 = "Every day holds a"

batch.append(torch.tensor(tokenizer.encode(txt1)))
batch.append(torch.tensor(tokenizer.encode(txt2)))
batch = torch.stack(batch, dim=0)
print(batch)
```

    tensor([[6109, 3626, 6100,  345],
            [6109, 1110, 6622,  257]])



```python
torch.manual_seed(123)
model = DummyGPTModel(GPT_CONFIG_124M)

logits = model(batch)
print("Output shape:", logits.shape)
print(logits)
```

    Output shape: torch.Size([2, 4, 50257])
    tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],
             [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],
             [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],
             [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],
    
            [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],
             [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],
             [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],
             [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],
           grad_fn=<UnsafeViewBackward0>)


---

**Note**

- If you are running this code on Windows or Linux, the resulting values above may look like as follows:
    
```
Output shape: torch.Size([2, 4, 50257])
tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],
         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],
         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],
         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],

        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],
         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],
         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],
         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],
       grad_fn=<UnsafeViewBackward0>)
```

- Since these are just random numbers, this is not a reason for concern, and you can proceed with the remainder of the chapter without issues

---

## 4.2 通过层归一化来规范化激活值
(Normalizing activations with layer normalization)

- 层归一化（也称为 LayerNorm，[Ba 等人，2016](https://arxiv.org/abs/1607.06450)）会将神经网络层的激活值归一化，使其均值为 0，方差为 1  
- 这能够稳定训练过程，并加速收敛到有效权重  
- 在 Transformer 块中，层归一化会在多头注意力模块的前后进行应用，我们将在后面实现；它还会应用在最终输出层之前

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp" width="400px">

- 让我们通过一个简单的神经网络层传递一个小的输入样本，来看看层归一化是如何工作的：


```python
torch.manual_seed(123)

# create 2 training examples with 5 dimensions (features) each
batch_example = torch.randn(2, 5) 

layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())
out = layer(batch_example)
print(out)
```

    tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],
            [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],
           grad_fn=<ReluBackward0>)


- 让我们计算上面两个输入的均值和方差：


```python
mean = out.mean(dim=-1, keepdim=True)
var = out.var(dim=-1, keepdim=True)

print("Mean:\n", mean)
print("Variance:\n", var)
```

    Mean:
     tensor([[0.1324],
            [0.2170]], grad_fn=<MeanBackward1>)
    Variance:
     tensor([[0.0231],
            [0.0398]], grad_fn=<VarBackward0>)


- 归一化是独立应用于两个输入（行）的；使用 `dim=-1` 会在最后一个维度（在这里是特征维度）上进行计算，而不是在行维度上进行

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/06.webp" width="400px">

- 减去均值并除以方差的平方根（标准差）可以使输入在列（特征）维度上达到均值为 0、方差为 1 的效果：


```python
out_norm = (out - mean) / torch.sqrt(var)
print("Normalized layer outputs:\n", out_norm)

mean = out_norm.mean(dim=-1, keepdim=True)
var = out_norm.var(dim=-1, keepdim=True)
print("Mean:\n", mean)
print("Variance:\n", var)
```

    Normalized layer outputs:
     tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],
            [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],
           grad_fn=<DivBackward0>)
    Mean:
     tensor([[-5.9605e-08],
            [ 1.9868e-08]], grad_fn=<MeanBackward1>)
    Variance:
     tensor([[1.0000],
            [1.0000]], grad_fn=<VarBackward0>)


- 每个输入的均值被调整为 0，方差调整为 1；为提升可读性，我们可以禁用 PyTorch 的科学计数法表示：


```python
torch.set_printoptions(sci_mode=False)
print("Mean:\n", mean)
print("Variance:\n", var)
```

    Mean:
     tensor([[    -0.0000],
            [     0.0000]], grad_fn=<MeanBackward1>)
    Variance:
     tensor([[1.0000],
            [1.0000]], grad_fn=<VarBackward0>)


- 上述步骤中，我们对每个输入的特征进行了归一化处理  
- 现在，基于相同的思想，我们可以实现一个 `LayerNorm` 类：


```python
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift
```

**缩放和平移**

- 注意，除了通过减去均值和除以方差进行归一化外，我们还增加了两个可训练参数：`scale` 和 `shift` 参数
- 初始时，`scale`（乘以 1）和 `shift`（加 0）值不会产生任何效果；不过，`scale` 和 `shift` 是可训练参数，当模型确定进行调整可以提高训练任务的性能时，LLM 会在训练过程中自动调整这两个参数
- 这使得模型可以学习到适合数据的适当缩放和平移操作
- 另外，我们在计算方差的平方根之前添加了一个小值（`eps`），以防止当方差为 0 时出现除以零的错误

**带偏方差**

- 在上面的方差计算中，将 `unbiased=False` 表示采用公式 $\frac{\sum_i (x_i - \bar{x})^2}{n}$ 来计算方差，其中 n 是样本大小（此处为特征数或列数）；该公式不包括贝塞尔修正（即分母中使用 `n-1`），因此提供了方差的有偏估计
- 对于 LLM 来说，嵌入维度 `n` 非常大，因此使用 n 和 `n-1` 之间的差异可以忽略不计
- 然而，由于 GPT-2 的归一化层在训练时使用了有偏方差，因此我们也采用了此设置，以便后续章节中加载的预训练权重具有兼容性

- 现在让我们实际尝试 `LayerNorm`：


```python
ln = LayerNorm(emb_dim=5)
out_ln = ln(batch_example)
```


```python
mean = out_ln.mean(dim=-1, keepdim=True)
var = out_ln.var(dim=-1, unbiased=False, keepdim=True)

print("Mean:\n", mean)
print("Variance:\n", var)
```

    Mean:
     tensor([[    -0.0000],
            [     0.0000]], grad_fn=<MeanBackward1>)
    Variance:
     tensor([[1.0000],
            [1.0000]], grad_fn=<VarBackward0>)


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/07.webp" width="400px">

## 4.3 实现带有 GELU 激活的前馈网络
(Implementing a feed forward network with GELU activations)

在本节中，我们实现一个用于 LLM 的 transformer 块中的小型神经网络子模块。我们从激活函数开始。

在深度学习中，ReLU（整流线性单元）激活函数因其简单性和在多种神经网络架构中的有效性而被广泛使用。然而在 LLM 中，除了传统的 ReLU，还会使用其他类型的激活函数，其中两个值得注意的例子是 GELU（高斯误差线性单元）和 SwiGLU（Swish 门控线性单元）。

与简单的分段线性 ReLU 不同，GELU 和 SwiGLU 是更复杂的平滑激活函数，分别结合了高斯和 sigmoid 门控单元，能够为深度学习模型提供更好的性能。

- GELU（高斯误差线性单元）[Hendrycks 和 Gimpel 2016](https://arxiv.org/abs/1606.08415)可以通过多种方式实现，准确的版本定义为：$\text{GELU}(x) = x \cdot \Phi(x)$，其中 $\Phi(x)$ 是标准高斯分布的累积分布函数。

- 在实践中，通常实现一个计算上更便宜的近似方法：  
  $\text{GELU}(x) \approx 0.5 \cdot x \cdot \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \cdot \left(x + 0.044715 \cdot x^3\right)\right]\right)$  
  （原始的 GPT-2 模型也是使用这个近似方法进行训练的）。


```python
class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) * 
            (x + 0.044715 * torch.pow(x, 3))
        ))
```


```python
import matplotlib.pyplot as plt

gelu, relu = GELU(), nn.ReLU()

# Some sample data
x = torch.linspace(-3, 3, 100)
y_gelu, y_relu = gelu(x), relu(x)

plt.figure(figsize=(8, 3))
for i, (y, label) in enumerate(zip([y_gelu, y_relu], ["GELU", "ReLU"]), 1):
    plt.subplot(1, 2, i)
    plt.plot(x, y)
    plt.title(f"{label} activation function")
    plt.xlabel("x")
    plt.ylabel(f"{label}(x)")
    plt.grid(True)

plt.tight_layout()
plt.show()
```


    
![png](output_41_0.png)
    


- 如我们所见，ReLU 是一个分段线性函数，当输入为正时，直接输出输入值；当输入为负时，输出为零。
- GELU 是一个平滑的非线性函数，近似于 ReLU，但对于负值具有非零梯度（除非输入大约为 -0.75）。

接下来，我们将实现一个小型神经网络模块 `FeedForward`，这个模块将在 LLM 的 transformer 块中使用：


```python
class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)
```


```python
print(GPT_CONFIG_124M["emb_dim"])
```

    768


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/09.webp?12" width="400px">


```python
ffn = FeedForward(GPT_CONFIG_124M)

# input shape: [batch_size, num_token, emb_size]
x = torch.rand(2, 3, 768) 
out = ffn(x)
print(out.shape)
```

    torch.Size([2, 3, 768])


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp" width="400px">

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/11.webp" width="400px">

## 4.4 添加残差连接
(Adding shortcut connections)

- 接下来，让我们谈谈快捷连接的概念，也叫跳跃连接或残差连接。
- 最初，快捷连接是在深度网络中为了解决计算机视觉中的梯度消失问题而提出的（残差网络）。
- 快捷连接为梯度流经网络创造了一个替代的较短路径。
- 这通过将一层的输出加到后续层的输出上来实现，通常跳过一个或多个中间层。
- 让我们用一个小示例网络来说明这个概念：

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/12.webp?123" width="400px">

- In code, it looks like this:


```python
class ExampleDeepNeuralNetwork(nn.Module):
    def __init__(self, layer_sizes, use_shortcut):
        super().__init__()
        self.use_shortcut = use_shortcut
        self.layers = nn.ModuleList([
            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())
        ])

    def forward(self, x):
        for layer in self.layers:
            # Compute the output of the current layer
            layer_output = layer(x)
            # Check if shortcut can be applied
            if self.use_shortcut and x.shape == layer_output.shape:
                x = x + layer_output
            else:
                x = layer_output
        return x


def print_gradients(model, x):
    # Forward pass
    output = model(x)
    target = torch.tensor([[0.]])

    # Calculate loss based on how close the target
    # and output are
    loss = nn.MSELoss()
    loss = loss(output, target)
    
    # Backward pass to calculate the gradients
    loss.backward()

    for name, param in model.named_parameters():
        if 'weight' in name:
            # Print the mean absolute gradient of the weights
            print(f"{name} has gradient mean of {param.grad.abs().mean().item()}")
```

- 首先，我们在 **没有** 快捷连接的情况下打印梯度值：


```python
layer_sizes = [3, 3, 3, 3, 3, 1]  

sample_input = torch.tensor([[1., 0., -1.]])

torch.manual_seed(123)
model_without_shortcut = ExampleDeepNeuralNetwork(
    layer_sizes, use_shortcut=False
)
print_gradients(model_without_shortcut, sample_input)
```

    layers.0.0.weight has gradient mean of 0.00020173587836325169
    layers.1.0.weight has gradient mean of 0.00012011159560643137
    layers.2.0.weight has gradient mean of 0.0007152039906941354
    layers.3.0.weight has gradient mean of 0.0013988736318424344
    layers.4.0.weight has gradient mean of 0.005049645435065031


- 接下来，我们在 **有** 快捷连接的情况下打印梯度值：


```python
torch.manual_seed(123)
model_with_shortcut = ExampleDeepNeuralNetwork(
    layer_sizes, use_shortcut=True
)
print_gradients(model_with_shortcut, sample_input)
```

    layers.0.0.weight has gradient mean of 0.22169792652130127
    layers.1.0.weight has gradient mean of 0.20694106817245483
    layers.2.0.weight has gradient mean of 0.32896995544433594
    layers.3.0.weight has gradient mean of 0.2665732204914093
    layers.4.0.weight has gradient mean of 1.3258540630340576


- 正如我们从上面的输出可以看到的，快捷连接防止了梯度在早期层（接近 `layer.0`）中消失
- 在接下来的实现中，我们将在实现 Transformer 块时使用这个快捷连接的概念

## 4.5 在 Transformer 块中连接注意力层和线性层
(Connecting attention and linear layers in a transformer block)

- 在本节中，我们将前面提到的概念结合成一个所谓的 Transformer 块。
- 一个 Transformer 块将上一章中的因果多头注意力模块与线性层、我们之前实现的前馈神经网络结合起来。
- 此外，Transformer 块还使用了 dropout 和快捷连接。


```python
from previous_chapters import MultiHeadAttention


class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"], 
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut  # Add the original input back

        return x
```

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp?1" width="400px">

- 假设我们有两个输入样本，每个样本包含6个标记，每个标记是一个768维的嵌入向量；那么这个 Transformer 块会首先应用自注意力机制，随后是线性层，从而生成一个相似大小的输出。
- 你可以将这个输出视为我们在上一章讨论的上下文向量的增强版本。


```python
torch.manual_seed(123)

x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]
block = TransformerBlock(GPT_CONFIG_124M)
output = block(x)

print("Input shape:", x.shape)
print("Output shape:", output.shape)
```

    Input shape: torch.Size([2, 4, 768])
    Output shape: torch.Size([2, 4, 768])


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/14.webp?1" width="400px">

## 4.6 编码GPT模型
(Coding the GPT model)

- 我们已经快完成了：现在让我们将 Transformer 块插入到本章开头编写的架构中，以便获得一个可用的 GPT 架构。
- 请注意，Transformer 块会被重复多次；以最小的 124M GPT-2 模型为例，我们将其重复 12 次。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp" width="400px">

- 对应的代码实现，其中 `cfg["n_layers"] = 12`：


```python
class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        
        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])
        
        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(
            cfg["emb_dim"], cfg["vocab_size"], bias=False
        )

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits
```

- 使用124M参数模型的配置，我们现在可以如下实例化这个GPT模型，并使用随机初始化权重：


```python
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)

out = model(batch)
print("Input batch:\n", batch)
print("\nOutput shape:", out.shape)
print(out)
```

    Input batch:
     tensor([[6109, 3626, 6100,  345],
            [6109, 1110, 6622,  257]])
    
    Output shape: torch.Size([2, 4, 50257])
    tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],
             [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],
             [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],
             [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],
    
            [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],
             [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],
             [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],
             [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],
           grad_fn=<UnsafeViewBackward0>)


- 我们将在下一章训练这个模型。
- 然而，关于它的大小，值得注意的是，我们之前称它为124M参数模型；我们可以通过以下方式重新确认这个数字：


```python
total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters: {total_params:,}")
```

    Total number of parameters: 163,009,536


- 如上所示，模型实际上有163M参数，而不是124M参数；为什么会这样？
- 在原始的GPT-2论文中，研究人员采用了**权重共享**（weight tying）技术，这意味着他们将token嵌入层（`tok_emb`）用作输出层，也就是通过将`self.out_head.weight = self.tok_emb.weight`来共享权重。
- Token嵌入层将50,257维的one-hot编码输入token映射到768维的嵌入表示。
- 输出层则将768维的嵌入再次映射回50,257维的表示，以便我们能够将它们转换回单词（更多细节将在下一节讨论）。
- 因此，嵌入层和输出层具有相同数量的权重参数，这可以通过它们权重矩阵的形状看到。

- 但有一点需要注意：我们之前将其称为124M参数的模型，现在可以通过以下方式确认这个数字：


```python
print("Token embedding layer shape:", model.tok_emb.weight.shape)
print("Output layer shape:", model.out_head.weight.shape)
```

    Token embedding layer shape: torch.Size([50257, 768])
    Output layer shape: torch.Size([50257, 768])


- 在原始的GPT-2论文中，研究人员将token嵌入矩阵重用作为输出矩阵。
- 因此，如果我们从总参数数目中减去输出层的参数数量，我们就会得到124M参数的模型：


```python
total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())
print(f"Number of trainable parameters considering weight tying: {total_params_gpt2:,}")
```

    Number of trainable parameters considering weight tying: 124,412,160


- 实际上，我发现训练没有权重共享的模型更容易，这也是为什么我们在这里没有实现它。
- 然而，我们会在第5章加载预训练权重时重新回顾并应用这种权重共享的思想。
- 最后，我们可以按如下方式计算模型的内存需求，这可以作为一个有用的参考点：


```python
# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)
total_size_bytes = total_params * 4

# Convert to megabytes
total_size_mb = total_size_bytes / (1024 * 1024)

print(f"Total size of the model: {total_size_mb:.2f} MB")
```

    Total size of the model: 621.83 MB


- 练习：你可以尝试以下在[GPT-2论文](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C)中提到的其他配置。

    - **GPT2-small** (the 124M configuration we already implemented):
        - "emb_dim" = 768
        - "n_layers" = 12
        - "n_heads" = 12

    - **GPT2-medium:**
        - "emb_dim" = 1024
        - "n_layers" = 24
        - "n_heads" = 16
    
    - **GPT2-large:**
        - "emb_dim" = 1280
        - "n_layers" = 36
        - "n_heads" = 20
    
    - **GPT2-XL:**
        - "emb_dim" = 1600
        - "n_layers" = 48
        - "n_heads" = 25

## 4.7 生成文本
(Generating text)

- 像我们上面实现的GPT模型这样的LLM（大语言模型）用于一次生成一个词。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/16.webp" width="400px">

- 以下的 `generate_text_simple` 函数实现了贪心解码，这是生成文本的一种简单且快速的方法。
- 在贪心解码中，模型在每一步选择具有最高概率的词（或标记）作为下一个输出（最高的logit对应于最高的概率，因此我们实际上不必显式地计算softmax函数）。
- 在下一章中，我们将实现一个更先进的 `generate_text` 函数。
- 下图展示了GPT模型在给定输入上下文的情况下如何生成下一个词标记。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/17.webp" width="600px">


```python
def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idx is (batch, n_tokens) array of indices in the current context
    for _ in range(max_new_tokens):
        
        # Crop current context if it exceeds the supported context size
        # E.g., if LLM supports only 5 tokens, and the context size is 10
        # then only the last 5 tokens are used as context
        idx_cond = idx[:, -context_size:]
        
        # Get the predictions
        with torch.no_grad():
            logits = model(idx_cond)
        
        # Focus only on the last time step
        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)
        logits = logits[:, -1, :]  

        # Apply softmax to get probabilities
        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)

        # Get the idx of the vocab entry with the highest probability value
        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)

        # Append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx
```

- 上面的 `generate_text_simple` 实现了一个迭代过程，每次生成一个标记（token）。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/18.webp" width="600px">

- 让我们准备一个输入示例：


```python
start_context = "Hello, I am"

encoded = tokenizer.encode(start_context)
print("encoded:", encoded)

encoded_tensor = torch.tensor(encoded).unsqueeze(0)
print("encoded_tensor.shape:", encoded_tensor.shape)
```

    encoded: [15496, 11, 314, 716]
    encoded_tensor.shape: torch.Size([1, 4])



```python
model.eval() # disable dropout

out = generate_text_simple(
    model=model,
    idx=encoded_tensor, 
    max_new_tokens=6, 
    context_size=GPT_CONFIG_124M["context_length"]
)

print("Output:", out)
print("Output length:", len(out[0]))
```

    Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])
    Output length: 10


- 去除批处理维度并转换回文本：


```python
decoded_text = tokenizer.decode(out.squeeze(0).tolist())
print(decoded_text)
```

    Hello, I am Featureiman Byeswickattribute argue


- 请注意，模型尚未经过训练；因此，上面的输出文本是随机的。
- 我们将在下一章对模型进行训练。

## 总结

- 请查看 [./gpt.py](./gpt.py) 脚本，这是一个包含我们在本 Jupyter Notebook 中实现的 GPT 模型的独立脚本。
- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到练习的解决方案。
