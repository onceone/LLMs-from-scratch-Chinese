

# 第6章：用于文本分类的微调

```python
from importlib.metadata import version

pkgs = ["matplotlib",
        "numpy",
        "tiktoken",
        "torch",
        "tensorflow", # For OpenAI's pretrained weights
        "pandas"      # Dataset loading
       ]
for p in pkgs:
    print(f"{p} version: {version(p)}")
```

    matplotlib version: 3.9.0
    numpy version: 1.26.4
    tiktoken version: 0.7.0
    torch version: 2.4.0
    tensorflow version: 2.16.1
    pandas version: 2.2.2


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/chapter-overview.webp" width=500px>

## 6.1 微调的不同类别

- 本节没有代码

- 最常见的微调方式有两种：指令微调和分类微调
- 指令微调，下一章的主题，如下图所示

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/instructions.webp" width=500px>

- 本章讨论的分类微调（classification finetuning）是一种你可能已经熟悉的过程，如果你有机器学习的背景的话——它类似于训练卷积神经网络来分类手写数字，例如。
- 在分类微调中，我们有一个特定数量的类别标签（例如，“垃圾邮件”和“非垃圾邮件”），模型可以输出这些类别。
- 一个经过分类微调的模型只能预测它在训练中见过的类别（例如，“垃圾邮件”或“非垃圾邮件”），而一个经过指令微调的模型通常可以执行许多任务。
- 我们可以将分类微调的模型看作是一个非常专业化的模型；实际上，创建一个专门化的模型比创建一个能够在许多不同任务上表现良好的通用模型要容易得多。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/spam-non-spam.webp" width=500px>

## 6.2 准备数据集

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-1.webp" width=500px>

- 本节准备用于分类微调的数据集
- 我们使用一个由垃圾短信和非垃圾短信组成的数据集来微调 LLM，以便对这些短信进行分类
- 首先，我们下载并解压数据集


```python
import urllib.request
import zipfile
import os
from pathlib import Path

url = "https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
zip_path = "sms_spam_collection.zip"
extracted_path = "sms_spam_collection"
data_file_path = Path(extracted_path) / "SMSSpamCollection.tsv"

def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):
    if data_file_path.exists():
        print(f"{data_file_path} already exists. Skipping download and extraction.")
        return

    # Downloading the file
    with urllib.request.urlopen(url) as response:
        with open(zip_path, "wb") as out_file:
            out_file.write(response.read())

    # Unzipping the file
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extracted_path)

    # Add .tsv file extension
    original_file_path = Path(extracted_path) / "SMSSpamCollection"
    os.rename(original_file_path, data_file_path)
    print(f"File downloaded and saved as {data_file_path}")

download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)
```

    File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv


- 数据集以制表符分隔的文本文件形式保存，我们可以将其加载到 pandas DataFrame 中


```python
import pandas as pd

df = pd.read_csv(data_file_path, sep="\t", header=None, names=["Label", "Text"])
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Label</th>
      <th>Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5567</th>
      <td>spam</td>
      <td>This is the 2nd time we have tried 2 contact u...</td>
    </tr>
    <tr>
      <th>5568</th>
      <td>ham</td>
      <td>Will ü b going to esplanade fr home?</td>
    </tr>
    <tr>
      <th>5569</th>
      <td>ham</td>
      <td>Pity, * was in mood for that. So...any other s...</td>
    </tr>
    <tr>
      <th>5570</th>
      <td>ham</td>
      <td>The guy did some bitching but I acted like i'd...</td>
    </tr>
    <tr>
      <th>5571</th>
      <td>ham</td>
      <td>Rofl. Its true to its name</td>
    </tr>
  </tbody>
</table>
<p>5572 rows × 2 columns</p>
</div>



- 当我们检查类别分布时，我们看到数据中“ham”（即“非垃圾邮件”）比“spam”更频繁


```python
print(df["Label"].value_counts())
```

    Label
    ham     4825
    spam     747
    Name: count, dtype: int64


- 为了简化操作，并且考虑到我们更倾向于使用较小的数据集以便于教学（这样可以加快 finetune LLM 的速度），我们对数据集进行了下采样（undersample），使其包含来自每个类别的747个实例
- （除了下采样外，还有多种处理类别不平衡的方法，但这些内容超出了本书的范畴；您可以在 [`imbalanced-learn` 用户指南](https://imbalanced-learn.org/stable/user_guide.html) 中找到更多示例和信息）


```python
def create_balanced_dataset(df):
    
    # Count the instances of "spam"
    num_spam = df[df["Label"] == "spam"].shape[0]
    
    # Randomly sample "ham" instances to match the number of "spam" instances
    ham_subset = df[df["Label"] == "ham"].sample(num_spam, random_state=123)
    
    # Combine ham "subset" with "spam"
    balanced_df = pd.concat([ham_subset, df[df["Label"] == "spam"]])

    return balanced_df

balanced_df = create_balanced_dataset(df)
print(balanced_df["Label"].value_counts())
```

    Label
    ham     747
    spam    747
    Name: count, dtype: int64


- 接下来，我们将字符串类型的类别标签 "ham" 和 "spam" 转换为整数类别标签 0 和 1：


```python
balanced_df["Label"] = balanced_df["Label"].map({"ham": 0, "spam": 1})
```

- 现在，让我们定义一个函数，随机将数据集划分为训练集、验证集和测试集：


```python
def random_split(df, train_frac, validation_frac):
    # Shuffle the entire DataFrame
    df = df.sample(frac=1, random_state=123).reset_index(drop=True)

    # Calculate split indices
    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)

    # Split the DataFrame
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]

    return train_df, validation_df, test_df

train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)
# Test size is implied to be 0.2 as the remainder

train_df.to_csv("train.csv", index=None)
validation_df.to_csv("validation.csv", index=None)
test_df.to_csv("test.csv", index=None)
```

## 6.3 创建数据加载器
(Creating data loaders)

- 注意，文本(text messages)的长度不同；如果我们想将多个训练样本合并成一个批次，我们必须：
  1. 将所有消息截断到数据集或批次中最短消息的长度，或者
  2. 将所有消息填充到数据集或批次中最长消息的长度

- 我们选择了选项 2，将所有消息填充到数据集中的最长消息长度
- 为此，我们使用 `<|endoftext|>` 作为填充标记，如第 2 章所讨论的

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/pad-input-sequences.webp?123" width=500px>


```python
import tiktoken

tokenizer = tiktoken.get_encoding("gpt2")
print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))
```

    [50256]


- 以下是 `SpamDataset` 类，它会识别训练数据集中最长的序列，并将填充标记添加到其他序列中，以使它们匹配该序列的长度。


```python
import torch
from torch.utils.data import Dataset


class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        self.data = pd.read_csv(csv_file)

        # Pre-tokenize texts
        self.encoded_texts = [
            tokenizer.encode(text) for text in self.data["Text"]
        ]

        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            # Truncate sequences if they are longer than max_length
            self.encoded_texts = [
                encoded_text[:self.max_length]
                for encoded_text in self.encoded_texts
            ]

        # Pad sequences to the longest sequence
        self.encoded_texts = [
            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))
            for encoded_text in self.encoded_texts
        ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self):
        max_length = 0
        for encoded_text in self.encoded_texts:
            encoded_length = len(encoded_text)
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length
```


```python
train_dataset = SpamDataset(
    csv_file="train.csv",
    max_length=None,
    tokenizer=tokenizer
)

print(train_dataset.max_length)
```

    120


- 我们还将验证集和测试集填充到最长的训练序列。
- 请注意，验证集和测试集中的样本如果长度超过了最长的训练示例，将通过 `encoded_text[:self.max_length]` 进行截断，这在 `SpamDataset` 代码中已实现。
- 这种行为是完全可选的，如果我们在验证集和测试集的情况下将 `max_length=None`，也能很好地工作。


```python
val_dataset = SpamDataset(
    csv_file="validation.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)
test_dataset = SpamDataset(
    csv_file="test.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)
```

- 接下来，我们使用数据集实例化数据加载器，这与之前章节中创建数据加载器的方法类似。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/batch.webp" width=500px>


```python
from torch.utils.data import DataLoader

num_workers = 0
batch_size = 8

torch.manual_seed(123)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)
```

- 作为验证步骤，我们遍历数据加载器，确保每个批次包含8个训练示例，并且每个训练示例由120个标记（tokens）组成。


```python
print("Train loader:")
for input_batch, target_batch in train_loader:
    pass

print("Input batch dimensions:", input_batch.shape)
print("Label batch dimensions", target_batch.shape)
```

    Train loader:
    Input batch dimensions: torch.Size([8, 120])
    Label batch dimensions torch.Size([8])


- 最后，让我们打印每个数据集中的总批次数量。


```python
print(f"{len(train_loader)} training batches")
print(f"{len(val_loader)} validation batches")
print(f"{len(test_loader)} test batches")
```

    130 training batches
    19 validation batches
    38 test batches


## 6.4 初始化带有预训练权重的模型
(Initializing a model with pretrained weights)

- 在本节中，我们将初始化在上一章中使用过的预训练模型。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-2.webp" width=500px>


```python
CHOOSE_MODEL = "gpt2-small (124M)"
INPUT_PROMPT = "Every effort moves"

BASE_CONFIG = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "drop_rate": 0.0,        # Dropout rate
    "qkv_bias": True         # Query-key-value bias
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

assert train_dataset.max_length <= BASE_CONFIG["context_length"], (
    f"Dataset length {train_dataset.max_length} exceeds model's context "
    f"length {BASE_CONFIG['context_length']}. Reinitialize data sets with "
    f"`max_length={BASE_CONFIG['context_length']}`"
)
```


```python
from gpt_download import download_and_load_gpt2
from previous_chapters import GPTModel, load_weights_into_gpt

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval();
```

    checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 24.2kiB/s]
    encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 2.53MiB/s]
    hparams.json: 100%|█████████████████████████| 90.0/90.0 [00:00<00:00, 37.4kiB/s]
    model.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [00:24<00:00, 20.7MiB/s]
    model.ckpt.index: 100%|████████████████████| 5.21k/5.21k [00:00<00:00, 924kiB/s]
    model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 1.89MiB/s]
    vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.79MiB/s]


- 为了确保模型正确加载，让我们再次检查它是否能生成连贯的文本。


```python
from previous_chapters import (
    generate_text_simple,
    text_to_token_ids,
    token_ids_to_text
)


text_1 = "Every effort moves you"

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_1, tokenizer),
    max_new_tokens=15,
    context_size=BASE_CONFIG["context_length"]
)

print(token_ids_to_text(token_ids, tokenizer))
```

    Every effort moves you forward.
    
    The first step is to understand the importance of your work


- 在我们将模型微调为分类器之前，先来看看模型是否可以通过提示（prompting）直接分类垃圾邮件。


```python
text_2 = (
    "Is the following text 'spam'? Answer with 'yes' or 'no':"
    " 'You are a winner you have been specially"
    " selected to receive $1000 cash or a $2000 award.'"
)

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_2, tokenizer),
    max_new_tokens=23,
    context_size=BASE_CONFIG["context_length"]
)

print(token_ids_to_text(token_ids, tokenizer))
```

    Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'
    
    The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner


- 正如我们所见，模型在遵循指令方面表现不佳。
- 这是可以预期的，因为它只进行了预训练，而没有进行指令微调（指令微调将在下一章讨论）。

## 6.5 增加分类头
(Adding a classification head)

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/lm-head.webp" width=500px>

- 在这一部分，我们将修改预训练的 LLM，使其准备好进行分类微调。
- 首先，让我们看一下模型的架构。


```python
print(model)
```

    GPTModel(
      (tok_emb): Embedding(50257, 768)
      (pos_emb): Embedding(1024, 768)
      (drop_emb): Dropout(p=0.0, inplace=False)
      (trf_blocks): Sequential(
        (0): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (1): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (2): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (3): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (4): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (5): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (6): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (7): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (8): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (9): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (10): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
        (11): TransformerBlock(
          (att): MultiHeadAttention(
            (W_query): Linear(in_features=768, out_features=768, bias=True)
            (W_key): Linear(in_features=768, out_features=768, bias=True)
            (W_value): Linear(in_features=768, out_features=768, bias=True)
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ff): FeedForward(
            (layers): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Linear(in_features=3072, out_features=768, bias=True)
            )
          )
          (norm1): LayerNorm()
          (norm2): LayerNorm()
          (drop_resid): Dropout(p=0.0, inplace=False)
        )
      )
      (final_norm): LayerNorm()
      (out_head): Linear(in_features=768, out_features=50257, bias=False)
    )


- 上面，我们可以看到我们在第 4 章中实现的架构被整齐地展示出来。
- 我们的目标是替换并微调输出层。
- 为此，我们首先冻结模型，这意味着我们将所有层设为不可训练。


```python
for param in model.parameters():
    param.requires_grad = False
```

- 然后，我们替换输出层（`model.out_head`），该层最初将输入层映射到 50,257 维（即词汇表的大小）。
- 由于我们将模型微调为二分类任务（预测 2 个类别，“spam”和“not spam”），因此可以如下所示替换输出层，且该层默认可训练。
- 请注意，我们使用 `BASE_CONFIG["emb_dim"]`（在 `"gpt2-small (124M)"` 模型中等于 768）来使下面的代码更加通用。


```python
torch.manual_seed(123)

num_classes = 2
model.out_head = torch.nn.Linear(in_features=BASE_CONFIG["emb_dim"], out_features=num_classes)
```

- 从技术上讲，仅训练输出层就足够了。
- 然而，正如我在[《微调大型语言模型》](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)中发现的，实验表明，微调额外的层可以显著提高性能。
- 因此，我们还将最后一个 transformer 块以及连接最后一个 transformer 块与输出层的最终 `LayerNorm` 模块设为可训练。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/trainable.webp" width=500px>


```python
for param in model.trf_blocks[-1].parameters():
    param.requires_grad = True

for param in model.final_norm.parameters():
    param.requires_grad = True
```

- 我们仍然可以像以前的章节中一样使用这个模型。
- 例如，让我们给它一些文本输入。


```python
inputs = tokenizer.encode("Do you have time")
inputs = torch.tensor(inputs).unsqueeze(0)
print("Inputs:", inputs)
print("Inputs dimensions:", inputs.shape) # shape: (batch_size, num_tokens)
```

    Inputs: tensor([[5211,  345,  423,  640]])
    Inputs dimensions: torch.Size([1, 4])


- 与以前的章节不同的是，模型现在有两个输出维度，而不是 50,257。


```python
with torch.no_grad():
    outputs = model(inputs)

print("Outputs:\n", outputs)
print("Outputs dimensions:", outputs.shape) # shape: (batch_size, num_tokens, num_classes)
```

    Outputs:
     tensor([[[-1.5854,  0.9904],
             [-3.7235,  7.4548],
             [-2.2661,  6.6049],
             [-3.5983,  3.9902]]])
    Outputs dimensions: torch.Size([1, 4, 2])


- 如前几章所讨论的，对于每个输入的标记，模型会产生一个输出向量。
- 由于我们输入的是一个包含 4 个标记的文本样本，因此输出由 4 个 2 维的输出向量组成，如上所示。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/input-and-output.webp" width=500px>

- 在第 3 章，我们讨论了注意力机制，它将每个输入标记与其他所有输入标记连接起来。
- 在第 3 章中，我们还介绍了 GPT 类模型中使用的因果注意力掩码；该因果掩码使当前标记只能关注当前及之前的标记位置。
- 基于这种因果注意力机制，第 4 个（最后一个）标记包含所有标记中最多的信息，因为它是唯一一个包含所有其他标记信息的标记。
- 因此，我们特别关注这个最后的标记，并将其作为垃圾邮件分类任务的目标进行微调。


```python
print("Last output token:", outputs[:, -1, :])
```

    Last output token: tensor([[-3.5983,  3.9902]])


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/attention-mask.webp" width=200px>

## 6.6 计算分类损失和准确率
(Calculating the classification loss and accuracy)

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-3.webp?1" width=500px>

- 在解释损失计算之前，我们先简要看看模型的输出是如何转化为分类标签的。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/class-argmax.webp" width=600px>


```python
print("Last output token:", outputs[:, -1, :])
```

    Last output token: tensor([[-3.5983,  3.9902]])


- 与第五章类似，我们通过 `softmax` 函数将模型的输出（logits）转换为概率分数，然后通过 `argmax` 函数获取最大概率值的索引位置。


```python
probas = torch.softmax(outputs[:, -1, :], dim=-1)
label = torch.argmax(probas)
print("Class label:", label.item())
```

    Class label: 1


- 请注意，正如第五章所解释的，这里使用 `softmax` 函数是可选的，因为最大的输出值对应于最大的概率分数。


```python
logits = outputs[:, -1, :]
label = torch.argmax(logits)
print("Class label:", label.item())
```

    Class label: 1


- 我们可以应用这个概念来计算所谓的分类准确率，分类准确率计算的是在给定数据集中正确预测的百分比。
- 为了计算分类准确率，我们可以将前面基于 `argmax` 的预测代码应用于数据集中的所有示例，并按以下方式计算正确预测的比例：


```python
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)

            with torch.no_grad():
                logits = model(input_batch)[:, -1, :]  # Logits of last output token
            predicted_labels = torch.argmax(logits, dim=-1)

            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples
```

- 让我们应用这个函数来计算不同数据集的分类准确率：


```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Note:
# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,
# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).
# As of this writing, in PyTorch 2.4, the results obtained via CPU and MPS were identical.
# However, in earlier versions of PyTorch, you may observe different results when using MPS.

#if torch.cuda.is_available():
#    device = torch.device("cuda")
#elif torch.backends.mps.is_available():
#    device = torch.device("mps")
#else:
#    device = torch.device("cpu")
#print(f"Running on {device} device.")

model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes

torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader

train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)
val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)
test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")
```

    Training accuracy: 46.25%
    Validation accuracy: 45.00%
    Test accuracy: 48.75%


- 正如我们所看到的，预测准确率并不是很好，因为我们还没有对模型进行微调。

- 在我们开始微调（训练）之前，我们首先需要定义训练中要优化的损失函数
- 目标是最大化模型的垃圾邮件分类准确率；然而，分类准确率不是一个可微的函数
- 因此，我们使用交叉熵损失函数来间接最大化分类准确率（你可以在我免费提供的[《深度学习导论》](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression)课程的第8讲中了解更多关于这个话题的信息）

- `calc_loss_batch`函数与第5章中相同，唯一不同的是我们这里只关注优化最后一个token `model(input_batch)[:, -1, :]`，而不是所有tokens `model(input_batch)`


```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)[:, -1, :]  # Logits of last output token
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss
```

`calc_loss_loader` 函数与第五章中的完全相同。


```python
# Same as in chapter 5
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # Reduce the number of batches to match the total number of batches in the data loader
        # if num_batches exceeds the number of batches in the data loader
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
```

- 使用 `calc_loss_loader` 函数，我们在开始训练之前计算初始的训练集、验证集和测试集的损失。


```python
with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)
    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)

print(f"Training loss: {train_loss:.3f}")
print(f"Validation loss: {val_loss:.3f}")
print(f"Test loss: {test_loss:.3f}")
```

    Training loss: 2.453
    Validation loss: 2.583
    Test loss: 2.322


- 在下一节中，我们将训练模型，以改善损失值，从而提高分类准确性。

## 6.7 微调
(Finetuning the model on supervised data)

- 在本节中，我们定义并使用训练函数来提高模型的分类准确性。
- 以下的 `train_classifier_simple` 函数与我们在第五章用于预训练模型的 `train_model_simple` 函数几乎相同。
- 唯一的两个不同之处是：
  1. 我们现在跟踪看到的训练样本数（`examples_seen`），而不是看到的标记数。
  2. 我们在每个训练周期结束后计算准确度，而不是像以前那样在每个周期结束后打印一个样本文本。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/training-loop.webp?1" width=500px>


```python
# Overall the same as `train_model_simple` in chapter 5
def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter):
    # Initialize lists to track losses and examples seen
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad() # Reset loss gradients from previous batch iteration
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward() # Calculate loss gradients
            optimizer.step() # Update model weights using loss gradients
            examples_seen += input_batch.shape[0] # New: track examples instead of tokens
            global_step += 1

            # Optional evaluation step
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # Calculate accuracy after each epoch
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen
```

- `train_classifier_simple` 中使用的 `evaluate_model` 函数与我们在第五章中使用的函数相同。


```python
# Same as chapter 5
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss
```

- 在 M3 MacBook Air 笔记本电脑上，训练大约需要 5 分钟，而在 V100 或 A100 GPU 上，训练时间不到半分钟。


```python
import time

start_time = time.time()

torch.manual_seed(123)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)

num_epochs = 5
train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=50, eval_iter=5,
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```

    Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392
    Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637
    Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557
    Training accuracy: 70.00% | Validation accuracy: 72.50%
    Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489
    Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397
    Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353
    Training accuracy: 82.50% | Validation accuracy: 85.00%
    Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320
    Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306
    Training accuracy: 90.00% | Validation accuracy: 90.00%
    Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200
    Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132
    Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137
    Training accuracy: 100.00% | Validation accuracy: 97.50%
    Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143
    Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074
    Training accuracy: 100.00% | Validation accuracy: 97.50%
    Training completed in 5.38 minutes.


- 与第五章类似，我们使用 matplotlib 来绘制训练集和验证集的损失函数图。


```python
import matplotlib.pyplot as plt

def plot_values(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_values, label=f"Training {label}")
    ax1.plot(epochs_seen, val_values, linestyle="-.", label=f"Validation {label}")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(label.capitalize())
    ax1.legend()

    # Create a second x-axis for examples seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Examples seen")

    fig.tight_layout()  # Adjust layout to make room
    plt.savefig(f"{label}-plot.pdf")
    plt.show()
```


```python
epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))

plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)
```


    
![png](output_100_0.png)
    


- 上图中，基于损失函数的下降趋势，我们可以看出模型的学习效果良好。
- 此外，训练损失和验证损失非常接近，这表明模型并没有过拟合训练数据。
- 同样地，我们也可以绘制下图显示准确率。


```python
epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))
examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))

plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label="accuracy")
```


    
![png](output_102_0.png)
    


- 从上面的准确率图可以看出，模型在第4和第5轮训练后达到了相对较高的训练和验证准确率。
- 然而，我们需要记住，在之前的训练函数中，我们指定了 `eval_iter=5`，这意味着我们只在每5个批次后估算了一次训练和验证集的性能。
- 我们可以通过下面的代码计算整个数据集上的训练、验证和测试集的性能。


```python
train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")
```

    Training accuracy: 97.21%
    Validation accuracy: 97.32%
    Test accuracy: 95.67%


- 我们可以看到，训练集和验证集的表现几乎完全相同。
- 然而，从略低的测试集表现来看，我们可以发现模型对训练数据有轻微的过拟合，同时也对验证数据产生了过拟合，因为验证集在调优一些超参数（例如学习率）时被使用过。
- 这是正常现象，然而，这个差距可以通过增加模型的丢弃率（`drop_rate`）或优化器设置中的 `weight_decay` 来进一步缩小。

## 6.8 使用LLM作为垃圾短信分类器
(Using the LLM as a spam classifier)

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-4.webp" width=500px>

- 最后，让我们将微调后的 GPT 模型投入实际使用。
- 以下的 `classify_review` 函数实现了类似于我们之前实现的 `SpamDataset` 的数据预处理步骤。
- 然后，该函数返回模型预测的整数类别标签，并返回相应的类别名称。


```python
def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):
    model.eval()

    # Prepare inputs to the model
    input_ids = tokenizer.encode(text)
    supported_context_length = model.pos_emb.weight.shape[0]
    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake
    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)

    # Truncate sequences if they too long
    input_ids = input_ids[:min(max_length, supported_context_length)]

    # Pad sequences to the longest sequence
    input_ids += [pad_token_id] * (max_length - len(input_ids))
    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension

    # Model inference
    with torch.no_grad():
        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token
    predicted_label = torch.argmax(logits, dim=-1).item()

    # Return the classified result
    return "spam" if predicted_label == 1 else "not spam"
```

- 让我们在下面尝试几个例子。


```python
text_1 = (
    "You are a winner you have been specially"
    " selected to receive $1000 cash or a $2000 award."
)

print(classify_review(
    text_1, model, tokenizer, device, max_length=train_dataset.max_length
))
```

    spam



```python
text_2 = (
    "Hey, just wanted to check if we're still on"
    " for dinner tonight? Let me know!"
)

print(classify_review(
    text_2, model, tokenizer, device, max_length=train_dataset.max_length
))
```

    not spam


- 最后，让我们保存模型，以防以后需要重新使用它，而不必再次训练。


```python
torch.save(model.state_dict(), "review_classifier.pth")
```

- 然后，在新的会话中，我们可以按如下方式加载模型：


```python
model_state_dict = torch.load("review_classifier.pth", map_location=device, weights_only=True)
model.load_state_dict(model_state_dict)
```




    <All keys matched successfully>



## 总结

- 请参阅 [./gpt_class_finetune.py](./gpt_class_finetune.py) 脚本，这是一个用于分类微调的独立脚本。
- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到本章的习题解决方案。
- 此外，感兴趣的读者可以在 [附录 E](../../appendix-E) 中找到关于低秩适应（LoRA）参数高效训练的介绍。
