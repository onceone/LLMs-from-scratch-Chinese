

# 第7 章: 微调以遵循指令
(Finetuning To Follow Instructions)


```python
from importlib.metadata import version

pkgs = [
    "matplotlib",  # Plotting library
    "tiktoken",    # Tokenizer
    "torch",       # Deep learning library
    "tqdm",        # Progress bar
    "tensorflow",  # For OpenAI's pretrained weights
]
for p in pkgs:
    print(f"{p} version: {version(p)}")
```

    matplotlib version: 3.7.1
    tiktoken version: 0.7.0
    torch version: 2.4.0
    tqdm version: 4.66.4
    tensorflow version: 2.15.0


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/overview.webp?1" width=500px>

## 7.1 指令微调介绍
(Introduction to instruction finetuning)

- 在第五章中，我们看到预训练一个大型语言模型（LLM）涉及一个训练过程，在这个过程中，它学会一次生成一个词。
- 因此，预训练的LLM擅长文本补全，但不擅长遵循指令。
- 在这一章中，我们将教会LLM更好地遵循指令。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/instruction-following.webp" width=500px>

- 本章涵盖的主题总结如下图所示。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-1.webp?1" width=500px>

## 7.2 准备用于监督指令微调的数据集
(Preparing a dataset for supervised instruction finetuning)

- 我们将使用我为本章准备的指令数据集。


```python
import json
import os
import urllib


def download_and_load_file(file_path, url):

    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)
    else:
        with open(file_path, "r", encoding="utf-8") as file:
            text_data = file.read()

    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)

    return data


file_path = "instruction-data.json"
url = (
    "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch"
    "/main/ch07/01_main-chapter-code/instruction-data.json"
)

data = download_and_load_file(file_path, url)
print("Number of entries:", len(data))
```

    Number of entries: 1100


- 我们从上面的JSON文件加载的`data`列表中的每一项都是以下形式的字典：


```python
print("Example entry:\n", data[50])
```

    Example entry:
     {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': "The correct spelling is 'Occasion.'"}


- 请注意，`'input'` 字段可以为空：


```python
print("Another example entry:\n", data[999])
```

    Another example entry:
     {'instruction': "What is an antonym of 'complicated'?", 'input': '', 'output': "An antonym of 'complicated' is 'simple'."}


- 指令微调通常被称为“监督指令微调”，因为它涉及在一个数据集上训练模型，其中明确提供了输入-输出对。
- 有多种方式将条目格式化为LLM的输入；下图展示了两种示例格式，分别用于训练Alpaca（[链接](https://crfm.stanford.edu/2023/03/13/alpaca.html)）和Phi-3（[链接](https://arxiv.org/abs/2404.14219)）LLM。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/prompt-style.webp?1" width=500px>

- 在本章中，我们使用Alpaca风格的提示格式，这是指令微调的原始提示模板。
- 下面，我们将格式化输入，作为输入传递给LLM。


```python
def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text
```

- 带有输入字段的格式化响应如下所示：


```python
model_input = format_input(data[50])
desired_response = f"\n\n### Response:\n{data[50]['output']}"

print(model_input + desired_response)
```

    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    
    ### Instruction:
    Identify the correct spelling of the following word.
    
    ### Input:
    Ocassion
    
    ### Response:
    The correct spelling is 'Occasion.'


- 以下是没有输入字段的格式化响应：


```python
model_input = format_input(data[999])
desired_response = f"\n\n### Response:\n{data[999]['output']}"

print(model_input + desired_response)
```

    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    
    ### Instruction:
    What is an antonym of 'complicated'?
    
    ### Response:
    An antonym of 'complicated' is 'simple'.


- 最后，在下一节准备PyTorch数据加载器之前，我们将数据集划分为训练集、验证集和测试集。


```python
train_portion = int(len(data) * 0.85)  # 85% for training
test_portion = int(len(data) * 0.1)    # 10% for testing
val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation

train_data = data[:train_portion]
test_data = data[train_portion:train_portion + test_portion]
val_data = data[train_portion + test_portion:]
```


```python
print("Training set length:", len(train_data))
print("Validation set length:", len(val_data))
print("Test set length:", len(test_data))
```

    Training set length: 935
    Validation set length: 55
    Test set length: 110


## 7.3 将数据组织成训练批次
(Organizing data into training batches)

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-2.webp?1" width=500px>

- 我们将数据集批次处理分为几个步骤，如下图所示。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/detailed-batching.webp?1" width=500px>

- 首先，我们实现一个`InstructionDataset`类，它会预先对数据集中的所有输入进行分词，类似于第六章中的`SpamDataset`。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/pretokenizing.webp" width=500px>


```python
import torch
from torch.utils.data import Dataset


class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # Pre-tokenize texts
        self.encoded_texts = []
        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = f"\n\n### Response:\n{entry['output']}"
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)
```

- 与第六章类似，我们希望将多个训练示例收集到一个批次中，以加速训练；这需要将所有输入填充到相似的长度。
- 同样，和上一章一样，我们使用`<|endoftext|>`令牌作为填充令牌。


```python
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")

print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))
```

    [50256]


- 在第六章中，我们将数据集中的所有示例填充到相同的长度。
  - 在这里，我们采用更复杂的方法，开发了一个自定义的“collate”函数，并将其传递给数据加载器。
  - 这个自定义的collate函数会将每个批次中的训练示例填充到相同的长度（但不同的批次可以有不同的长度）。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/padding.webp" width=500px>


```python
def custom_collate_draft_1(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    # Find the longest sequence in the batch
    # and increase the max length by +1, which will add one extra
    # padding token below
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs
    inputs_lst = []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to batch_max_length
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        # Via padded[:-1], we remove the extra padded token
        # that has been added via the +1 setting in batch_max_length
        # (the extra padding token will be relevant in later codes)
        inputs = torch.tensor(padded[:-1])
        inputs_lst.append(inputs)

    # Convert list of inputs to tensor and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    return inputs_tensor
```


```python
inputs_1 = [0, 1, 2, 3, 4]
inputs_2 = [5, 6]
inputs_3 = [7, 8, 9]

batch = (
    inputs_1,
    inputs_2,
    inputs_3
)

print(custom_collate_draft_1(batch))
```

    tensor([[    0,     1,     2,     3,     4],
            [    5,     6, 50256, 50256, 50256],
            [    7,     8,     9, 50256, 50256]])


<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-4.webp?1" width=500px>

- 上面，我们只返回了LLM的输入；然而，对于LLM训练，我们还需要目标值。
- 类似于预训练LLM，目标是将输入向右平移1个位置，因此LLM学习预测下一个令牌。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/inputs-targets.webp?1" width=400px>


```python
def custom_collate_draft_2(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets
        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs to tensor and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor
```


```python
inputs, targets = custom_collate_draft_2(batch)
print(inputs)
print(targets)
```

    tensor([[    0,     1,     2,     3,     4],
            [    5,     6, 50256, 50256, 50256],
            [    7,     8,     9, 50256, 50256]])
    tensor([[    1,     2,     3,     4, 50256],
            [    6, 50256, 50256, 50256, 50256],
            [    8,     9, 50256, 50256, 50256]])


- 接下来，我们引入一个`ignore_index`值，将所有填充令牌的ID替换为一个新值；`ignore_index`的目的是在损失函数中忽略填充值（稍后会详细说明）。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-5.webp?1" width=500px>

- 具体来说，这意味着我们将对应于`50256`的令牌ID替换为`-100`，如下所示：

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ignore-index.webp?1" width=500px>

- （另外，我们还引入了`allowed_max_length`，以便在需要时限制样本的长度；如果你计划使用比GPT-2模型支持的1024个令牌上下文长度更长的自定义数据集，这将非常有用。）


```python
def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs and targets
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets

        # New: Replace all but the first padding tokens in targets by ignore_index
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index

        # New: Optionally truncate to maximum sequence length
        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs and targets to tensors and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)

    return inputs_tensor, targets_tensor
```


```python
inputs, targets = custom_collate_fn(batch)
print(inputs)
print(targets)
```

    tensor([[    0,     1,     2,     3,     4],
            [    5,     6, 50256, 50256, 50256],
            [    7,     8,     9, 50256, 50256]])
    tensor([[    1,     2,     3,     4, 50256],
            [    6, 50256,  -100,  -100,  -100],
            [    8,     9, 50256,  -100,  -100]])


- 让我们来看一下这个用`-100`替换的效果。
- 为了说明，假设我们有一个小的分类任务，包含两个类别标签0和1，类似于第六章的内容。
- 如果我们有以下的logits值（即模型最后一层的输出），我们可以计算如下的损失：


```python
logits_1 = torch.tensor(
    [[-1.0, 1.0],  # 1st training example
     [-0.5, 1.5]]  # 2nd training example
)
targets_1 = torch.tensor([0, 1])


loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)
print(loss_1)
```

    tensor(1.1269)


- 现在，加入一个新的训练示例将如预期的那样，影响损失值。


```python
logits_2 = torch.tensor(
    [[-1.0, 1.0],
     [-0.5, 1.5],
     [-0.5, 1.5]]  # New 3rd training example
)
targets_2 = torch.tensor([0, 1, 1])

loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)
print(loss_2)
```

    tensor(0.7936)


- 让我们看看如果将其中一个示例的类别标签替换为`-100`会发生什么。


```python
targets_3 = torch.tensor([0, 1, -100])

loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)
print(loss_3)
print("loss_1 == loss_3:", loss_1 == loss_3)
```

    tensor(1.1269)
    loss_1 == loss_3: tensor(True)


- 正如我们所看到的，在这3个训练示例上的最终损失与我们从2个训练示例中计算出的损失相同，这意味着交叉熵损失函数忽略了标签为`-100`的训练示例。
- 默认情况下，PyTorch的`cross_entropy(..., ignore_index=-100)`设置会忽略标签为`-100`的示例。
- 使用这个`-100`的`ignore_index`，我们可以忽略批次中填充的额外结束符（填充）令牌，这些令牌用于将训练示例填充到相同的长度。
- 然而，我们不希望忽略第一个结束符（填充）令牌（50256），因为它可以帮助LLM判断何时响应结束。

- 实际上，通常也会屏蔽掉对应于指令的目标令牌ID，如下图所示（这是完成本章后的推荐读者练习）。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp?1" width=600px>

## 7.4 为指令数据集创建数据加载器

- 在本节中，我们使用`InstructionDataset`类和`custom_collate_fn`函数来实例化训练、验证和测试数据加载器。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-3.webp?1" width=500px>

- 之前的`custom_collate_fn`函数的另一个附加细节是，我们现在直接将数据移动到目标设备（例如GPU），而不是在主训练循环中执行这一步，这提高了效率，因为当我们将`custom_collate_fn`作为数据加载器的一部分时，这个过程可以在后台进行。
- 通过使用Python `functools`标准库中的`partial`函数，我们创建了一个新的函数，预先填充了原始函数中的`device`参数。


```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Note:
# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,
# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).
# However, the resulting loss values may be slightly different.

#if torch.cuda.is_available():
#    device = torch.device("cuda")
#elif torch.backends.mps.is_available():
#    device = torch.device("mps")
#else:
#    device = torch.device("cpu")

print("Device:", device)
```

    Device: cuda



```python
from functools import partial

customized_collate_fn = partial(
    custom_collate_fn,
    device=device,
    allowed_max_length=1024
)
```

- 接下来，我们像之前章节一样实例化数据加载器，不同之处在于我们现在为批处理过程提供了自定义的collate函数。


```python
from torch.utils.data import DataLoader


num_workers = 0
batch_size = 8

torch.manual_seed(123)

train_dataset = InstructionDataset(train_data, tokenizer)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=True,
    drop_last=True,
    num_workers=num_workers
)
```


```python
val_dataset = InstructionDataset(val_data, tokenizer)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

test_dataset = InstructionDataset(test_data, tokenizer)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)
```

- 让我们看看结果输入和目标批次的维度是什么样的。


```python
print("Train loader:")
for inputs, targets in train_loader:
    print(inputs.shape, targets.shape)
```

    Train loader:
    torch.Size([8, 61]) torch.Size([8, 61])
    torch.Size([8, 76]) torch.Size([8, 76])
    torch.Size([8, 73]) torch.Size([8, 73])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 72]) torch.Size([8, 72])
    torch.Size([8, 80]) torch.Size([8, 80])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 62]) torch.Size([8, 62])
    torch.Size([8, 75]) torch.Size([8, 75])
    torch.Size([8, 62]) torch.Size([8, 62])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 77]) torch.Size([8, 77])
    torch.Size([8, 69]) torch.Size([8, 69])
    torch.Size([8, 79]) torch.Size([8, 79])
    torch.Size([8, 71]) torch.Size([8, 71])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 83]) torch.Size([8, 83])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 80]) torch.Size([8, 80])
    torch.Size([8, 71]) torch.Size([8, 71])
    torch.Size([8, 69]) torch.Size([8, 69])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 60]) torch.Size([8, 60])
    torch.Size([8, 59]) torch.Size([8, 59])
    torch.Size([8, 69]) torch.Size([8, 69])
    torch.Size([8, 63]) torch.Size([8, 63])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 76]) torch.Size([8, 76])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 71]) torch.Size([8, 71])
    torch.Size([8, 91]) torch.Size([8, 91])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 75]) torch.Size([8, 75])
    torch.Size([8, 89]) torch.Size([8, 89])
    torch.Size([8, 59]) torch.Size([8, 59])
    torch.Size([8, 88]) torch.Size([8, 88])
    torch.Size([8, 83]) torch.Size([8, 83])
    torch.Size([8, 83]) torch.Size([8, 83])
    torch.Size([8, 70]) torch.Size([8, 70])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 74]) torch.Size([8, 74])
    torch.Size([8, 76]) torch.Size([8, 76])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 75]) torch.Size([8, 75])
    torch.Size([8, 83]) torch.Size([8, 83])
    torch.Size([8, 69]) torch.Size([8, 69])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 60]) torch.Size([8, 60])
    torch.Size([8, 60]) torch.Size([8, 60])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 80]) torch.Size([8, 80])
    torch.Size([8, 71]) torch.Size([8, 71])
    torch.Size([8, 61]) torch.Size([8, 61])
    torch.Size([8, 58]) torch.Size([8, 58])
    torch.Size([8, 71]) torch.Size([8, 71])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 63]) torch.Size([8, 63])
    torch.Size([8, 87]) torch.Size([8, 87])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 71]) torch.Size([8, 71])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 71]) torch.Size([8, 71])
    torch.Size([8, 61]) torch.Size([8, 61])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 65]) torch.Size([8, 65])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 60]) torch.Size([8, 60])
    torch.Size([8, 72]) torch.Size([8, 72])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 70]) torch.Size([8, 70])
    torch.Size([8, 57]) torch.Size([8, 57])
    torch.Size([8, 72]) torch.Size([8, 72])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 62]) torch.Size([8, 62])
    torch.Size([8, 74]) torch.Size([8, 74])
    torch.Size([8, 80]) torch.Size([8, 80])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 70]) torch.Size([8, 70])
    torch.Size([8, 91]) torch.Size([8, 91])
    torch.Size([8, 61]) torch.Size([8, 61])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 80]) torch.Size([8, 80])
    torch.Size([8, 81]) torch.Size([8, 81])
    torch.Size([8, 74]) torch.Size([8, 74])
    torch.Size([8, 82]) torch.Size([8, 82])
    torch.Size([8, 63]) torch.Size([8, 63])
    torch.Size([8, 83]) torch.Size([8, 83])
    torch.Size([8, 68]) torch.Size([8, 68])
    torch.Size([8, 67]) torch.Size([8, 67])
    torch.Size([8, 77]) torch.Size([8, 77])
    torch.Size([8, 91]) torch.Size([8, 91])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 61]) torch.Size([8, 61])
    torch.Size([8, 75]) torch.Size([8, 75])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 78]) torch.Size([8, 78])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 64]) torch.Size([8, 64])
    torch.Size([8, 83]) torch.Size([8, 83])
    torch.Size([8, 66]) torch.Size([8, 66])
    torch.Size([8, 74]) torch.Size([8, 74])
    torch.Size([8, 69]) torch.Size([8, 69])


- 正如我们从上面的输出中看到的，所有批次的批大小都是8，但长度各不相同，这是预期的结果。
- 让我们还检查一下，输入中是否包含对应于令牌ID 50256的`<|endoftext|>`填充令牌，通过打印`inputs`批次中第一个训练示例的内容来确认这一点。


```python
print(inputs[0])
```

    tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,
              257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,
            21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,
              985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,
             5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,
              198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,
            50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],
           device='cuda:0')


- 同样，我们也可以通过可视化检查，确认目标中是否包含`-100`占位符令牌。


```python
print(targets[0])
```

    tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,
             2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,
            46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,
              576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,
              318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,
              464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,
             -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],
           device='cuda:0')


## 7.5 加载预训练的大型语言模型 (LLM)

- 在本节中，我们使用与第5章第5.5节和第6章第6.4节中相同的代码加载一个预训练的GPT模型。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-4.webp?1" width=500px>

- 然而，我们不是加载最小的1.24亿参数模型，而是加载具有3.55亿参数的中型版本，因为1.24亿参数的模型对于通过指令微调达到合理的质量结果来说太小了。


```python
from gpt_download import download_and_load_gpt2
from previous_chapters import GPTModel, load_weights_into_gpt


BASE_CONFIG = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "drop_rate": 0.0,        # Dropout rate
    "qkv_bias": True         # Query-key-value bias
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

CHOOSE_MODEL = "gpt2-medium (355M)"

BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
settings, params = download_and_load_gpt2(
    model_size=model_size,
    models_dir="gpt2"
)

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval();
```

    2024-07-25 02:22:49.969483: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
    2024-07-25 02:22:50.023103: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
    2024-07-25 02:22:50.023136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
    2024-07-25 02:22:50.024611: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
    2024-07-25 02:22:50.033304: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
    To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    2024-07-25 02:22:51.282247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
    checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 169kiB/s]
    encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.43MiB/s]
    hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 168kiB/s]
    model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [00:56<00:00, 25.0MiB/s]
    model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 16.5MiB/s]
    model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 1.96MiB/s]
    vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.53MiB/s]


- 在我们开始在下一节微调模型之前，让我们看看它在其中一个验证任务上的表现如何。


```python
torch.manual_seed(123)

input_text = format_input(val_data[0])
print(input_text)
```

    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    
    ### Instruction:
    Convert the active sentence to passive: 'The chef cooks the meal every day.'



```python
from previous_chapters import (
    generate,
    text_to_token_ids,
    token_ids_to_text
)

token_ids = generate(
    model=model,
    idx=text_to_token_ids(input_text, tokenizer),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
    eos_id=50256,
)
generated_text = token_ids_to_text(token_ids, tokenizer)
```

- 请注意，我们在之前章节中使用的`generate`函数返回的是输入和输出文本的组合，这在上一节中对于创建可读的文本非常方便。
- 为了隔离响应，我们可以从`generated_text`的开头减去指令的长度。


```python
response_text = (
    generated_text[len(input_text):]
    .replace("### Response:", "")
    .strip()
)
print(response_text)
```

    The chef cooks the meal every day.
    
    ### Instruction:
    
    Convert the active sentence to passive: 'The chef cooks the


- 正如我们所看到的，模型还没有能力遵循指令；它创建了一个“Response”部分，但它只是简单地重复了原始输入句子和指令。

## 7.6 在指令数据上微调LLM
(Finetuning the LLM on instruction data)

- 在本节中，我们对模型进行微调。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-5.webp?1" width=500px>

- 请注意，我们可以重用之前章节中使用的所有损失计算和训练函数。


```python
from previous_chapters import (
    calc_loss_loader,
    train_model_simple
)
```

- 在开始训练之前，让我们计算初始的训练集和验证集损失（与之前章节一样，目标是最小化损失）。


```python
model.to(device)

torch.manual_seed(123)

with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```

    Training loss: 3.82590970993042
    Validation loss: 3.761933755874634


- 请注意，由于我们使用的是一个更大的模型（3.55亿参数而不是1.24亿参数），所以训练的成本比之前章节稍高。
- 以下显示了不同设备的运行时间供参考（在兼容的GPU设备上运行此笔记本无需修改代码）。

<div style="text-align: left;">
    
| Model              | Device                | Runtime for 2 Epochs |
|--------------------|-----------------------|----------------------|
| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 minutes        |
| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 minutes        |
| gpt2-medium (355M) | GPU (L4)              | 1.83 minutes         |
| gpt2-medium (355M) | GPU (A100)            | 0.86 minutes         |
| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes         |
| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 minutes         |
| gpt2-small (124M)  | GPU (L4)              | 0.69 minutes         |
| gpt2-small (124M)  | GPU (A100)            | 0.39 minutes         |

</div>

- I ran this notebook using the `"gpt2-medium (355M)"` model


```python
import time

start_time = time.time()

torch.manual_seed(123)

optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)

num_epochs = 2

train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context=format_input(val_data[0]), tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```

    Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626
    Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102
    Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944
    Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906
    Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881
    Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859
    Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836
    Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808
    Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806
    Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789
    Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783
    Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763
    Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743
    Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735
    Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729
    Ep 1 (Step 000075): Train loss 0.569, Val loss 0.728
    Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725
    Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709
    Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691
    Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681
    Ep 1 (Step 000100): Train loss 0.503, Val loss 0.677
    Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670
    Ep 1 (Step 000110): Train loss 0.555, Val loss 0.666
    Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664
    Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:
    Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672
    Ep 2 (Step 000125): Train loss 0.451, Val loss 0.687
    Ep 2 (Step 000130): Train loss 0.447, Val loss 0.683
    Ep 2 (Step 000135): Train loss 0.405, Val loss 0.682
    Ep 2 (Step 000140): Train loss 0.409, Val loss 0.681
    Ep 2 (Step 000145): Train loss 0.369, Val loss 0.680
    Ep 2 (Step 000150): Train loss 0.382, Val loss 0.675
    Ep 2 (Step 000155): Train loss 0.413, Val loss 0.675
    Ep 2 (Step 000160): Train loss 0.415, Val loss 0.683
    Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686
    Ep 2 (Step 000170): Train loss 0.323, Val loss 0.681
    Ep 2 (Step 000175): Train loss 0.337, Val loss 0.669
    Ep 2 (Step 000180): Train loss 0.392, Val loss 0.656
    Ep 2 (Step 000185): Train loss 0.415, Val loss 0.657
    Ep 2 (Step 000190): Train loss 0.340, Val loss 0.648
    Ep 2 (Step 000195): Train loss 0.330, Val loss 0.634
    Ep 2 (Step 000200): Train loss 0.310, Val loss 0.634
    Ep 2 (Step 000205): Train loss 0.352, Val loss 0.630
    Ep 2 (Step 000210): Train loss 0.367, Val loss 0.630
    Ep 2 (Step 000215): Train loss 0.394, Val loss 0.635
    Ep 2 (Step 000220): Train loss 0.299, Val loss 0.648
    Ep 2 (Step 000225): Train loss 0.346, Val loss 0.661
    Ep 2 (Step 000230): Train loss 0.292, Val loss 0.659
    Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom
    Training completed in 1.84 minutes.


- 正如我们从上面的输出中看到的，模型训练得很好，训练损失和验证损失值都在下降。
- 此外，根据每个epoch后打印的响应文本，我们可以看到模型正确地遵循了指令，将输入句子`'The chef cooks the meal every day.'`转换为被动语态`'The meal is cooked every day by the chef.'`（我们将在后续章节中对响应进行适当的格式化和评估）。
- 最后，让我们看一下训练和验证损失曲线。


```python
from previous_chapters import plot_losses

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```


    
![png](output_91_0.png)
    


- 正如我们所看到的，损失在第一个epoch的开始时急剧下降，这意味着模型开始快速学习。
- 我们可以看到，在大约1个训练epoch后，模型出现了轻微的过拟合。

## 7.7 提取并保存响应

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-6.webp?1" width=500px>

- 在本节中，我们保存测试集的响应，以便在下一节进行评分。
- 我们还保存了模型的副本以供将来使用。
- 但首先，让我们简要查看一下微调后的模型生成的响应。


```python
torch.manual_seed(123)


for entry in test_data[:3]:

    input_text = format_input(entry)

    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = (
        generated_text[len(input_text):]
        .replace("### Response:", "")
        .strip()
)

    print(input_text)
    print(f"\nCorrect response:\n>> {entry['output']}")
    print(f"\nModel response:\n>> {response_text.strip()}")
    print("-------------------------------------")
```

    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    
    ### Instruction:
    Rewrite the sentence using a simile.
    
    ### Input:
    The car is very fast.
    
    Correct response:
    >> The car is as fast as lightning.
    
    Model response:
    >> The car is as fast as a bullet.
    -------------------------------------
    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    
    ### Instruction:
    What type of cloud is typically associated with thunderstorms?
    
    Correct response:
    >> The type of cloud typically associated with thunderstorms is cumulonimbus.
    
    Model response:
    >> The type of cloud associated with thunderstorms is a cumulus cloud.
    -------------------------------------
    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    
    ### Instruction:
    Name the author of 'Pride and Prejudice'.
    
    Correct response:
    >> Jane Austen.
    
    Model response:
    >> The author of 'Pride and Prejudice' is Jane Austen.
    -------------------------------------


- 正如我们从测试集指令、给定的响应和模型的响应中看到的，模型的表现相对较好。
- 第一和最后一条指令的答案显然是正确的。
- 第二个答案很接近；模型回答为“cumulus cloud”（积云），而不是“cumulonimbus”（雷暴云）（不过，值得注意的是，积云有可能发展成雷暴云，这种云可以产生雷暴）。
- 最重要的是，我们可以看到，模型评估不像上一章那样简单，我们只需计算正确的垃圾邮件/非垃圾邮件类别标签的百分比来获得分类准确率。
- 实际上，指令微调的LLM（如聊天机器人）通过多种方法进行评估：
  - 短答题和多项选择基准，如MMLU（“测量大规模多任务语言理解”，[链接](https://arxiv.org/abs/2009.03300)），用于测试模型的知识。
  - 与其他LLM的人工偏好比较，如LMSYS聊天机器人竞技场（[链接](https://arena.lmsys.org)）。
  - 自动化对话基准，使用另一个LLM（如GPT-4）来评估响应，如AlpacaEval（[链接](https://tatsu-lab.github.io/alpaca_eval/)）。

- 在下一节中，我们将采用类似AlpacaEval的方法，使用另一个LLM来评估我们模型的响应；但是，我们将使用自己的测试集，而不是使用公开的基准数据集。
- 为此，我们将模型的响应添加到`test_data`字典中，并保存为`"instruction-data-with-response.json"`文件以供记录保存，这样如果需要，我们可以在单独的Python会话中加载并分析它。


```python
from tqdm import tqdm

for i, entry in tqdm(enumerate(test_data), total=len(test_data)):

    input_text = format_input(entry)

    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = generated_text[len(input_text):].replace("### Response:", "").strip()

    test_data[i]["model_response"] = response_text


with open("instruction-data-with-response.json", "w") as file:
    json.dump(test_data, file, indent=4)  # "indent" for pretty-printing
```

    100%|██████████| 110/110 [01:11<00:00,  1.54it/s]


- 让我们再检查一下其中一条记录，看看模型的响应是否已正确添加到`test_data`字典中。


```python
print(test_data[0])
```

    {'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}


- 最后，我们还保存了模型，以防将来需要重新使用它。


```python
import re


file_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth"
torch.save(model.state_dict(), file_name)
print(f"Model saved as {file_name}")

# Load model via
# model.load_state_dict(torch.load("gpt2-medium355M-sft.pth"))
```

    Model saved as gpt2-medium355M-sft.pth


## 7.8 评估微调后的LLM
(Evaluating the finetuned LLM)

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-7.webp?1" width=500px>

- 在本节中，我们使用另一个更大的LLM自动化评估微调后的LLM响应。
- 具体来说，我们使用Meta AI的8亿参数Llama 3模型，这个模型经过指令微调，并且可以通过ollama本地运行（[链接](https://ollama.com)）。
- （或者，如果你更喜欢使用像GPT-4这样更强大的LLM，通过OpenAI API进行评估，请参考[llm-instruction-eval-openai.ipynb](../03_model-evaluation/llm-instruction-eval-openai.ipynb)笔记本）。

- Ollama是一个高效运行LLM的应用程序。
- 它是一个封装了llama.cpp的工具（[链接](https://github.com/ggerganov/llama.cpp)），该库使用纯C/C++实现LLM，旨在最大化效率。
- 请注意，它是一个用于使用LLM生成文本（推理）的工具，而不是用于训练或微调LLM的工具。
- 在运行下面的代码之前，请访问[https://ollama.com](https://ollama.com)安装ollama，并按照说明进行操作（例如，点击“下载”按钮并下载适合你操作系统的ollama应用程序）。

- 对于macOS和Windows用户，点击你下载的ollama应用程序；如果提示你安装命令行使用，选择“是”。
- Linux用户可以使用ollama网站提供的安装命令。

- 一般来说，在我们从命令行使用ollama之前，需要启动ollama应用程序，或者在一个单独的终端中运行`ollama serve`。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ollama-run.webp?1" width=700px>


- 在运行了ollama应用程序或在另一个终端中运行了`ollama serve`的情况下，在命令行执行以下命令来尝试8亿参数的Llama 3模型（该模型需要4.7 GB的存储空间，并将在你第一次执行该命令时自动下载）： 


```bash
# 8B model
ollama run llama3
```


The output looks like as follows

```
$ ollama run llama3
pulling manifest
pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB
pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB
pulling 8ab4849b038c... 100% ▕████████████████▏  254 B
pulling 577073ffcc6c... 100% ▕████████████████▏  110 B
pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B
verifying sha256 digest
writing manifest
removing any unused layers
success
```

- 请注意，`llama3`指的是经过指令微调的8亿参数Llama 3模型。

- 使用`ollama`运行`"llama3"`模型（一个8B参数模型）需要16 GB的RAM；如果你的机器不支持这一点，你可以尝试使用更小的模型，如3.8B参数的phi-3模型，通过设置`model = "phi-3"`，它只需要8 GB的RAM。

- 另外，如果你的机器支持，也可以使用更大的70亿参数Llama 3模型，只需将`llama3`替换为`llama3:70b`。

- 下载完成后，你将看到一个命令行提示符，允许你与模型进行对话。

- 尝试输入类似“What do llamas eat?”的提示，应该返回类似以下的输出： 



```
>>> What do llamas eat?
Llamas are ruminant animals, which means they have a four-chambered
stomach and eat plants that are high in fiber. In the wild, llamas
typically feed on:
1. Grasses: They love to graze on various types of grasses, including tall
grasses, wheat, oats, and barley.
```

- 你可以通过输入`/bye`来结束本次会话。

- 以下代码检查ollama会话是否正常运行，然后继续使用ollama评估我们在上一节中生成的测试集响应：


```python
import psutil

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running

ollama_running = check_if_running("ollama")

if not ollama_running:
    raise RuntimeError("Ollama not running. Launch ollama before proceeding.")
print("Ollama running:", check_if_running("ollama"))
```

    Ollama running: True



```python
# This cell is optional; it allows you to restart the notebook
# and only run section 7.7 without rerunning any of the previous code
import json
from tqdm import tqdm

file_path = "instruction-data-with-response.json"

with open(file_path, "r") as file:
    test_data = json.load(file)


def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text
```

- 现在，除了之前我们使用的ollama run命令与模型交互外，还可以通过其REST API在Python中与模型交互，以下是对应的函数。
- 在运行接下来的代码单元之前，请确保ollama仍在运行（之前的代码单元应该打印出"Ollama running: True"）。
- 然后，运行以下代码单元来查询模型：


```python
import urllib.request

def query_model(
    prompt,
    model="llama3",
    url="http://localhost:11434/api/chat"
):
    # Create the data payload as a dictionary
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "options": {     # Settings below are required for deterministic responses
            "seed": 123,
            "temperature": 0,
            "num_ctx": 2048
        }
    }


    # Convert the dictionary to a JSON formatted string and encode it to bytes
    payload = json.dumps(data).encode("utf-8")

    # Create a request object, setting the method to POST and adding necessary headers
    request = urllib.request.Request(
        url,
        data=payload,
        method="POST"
    )
    request.add_header("Content-Type", "application/json")

    # Send the request and capture the response
    response_data = ""
    with urllib.request.urlopen(request) as response:
        # Read and decode the response
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]

    return response_data


model = "llama3"
result = query_model("What do Llamas eat?", model)
print(result)
```

    Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:
    
    1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.
    2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.
    3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.
    4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.
    5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.
    
    In the wild, llamas might also eat:
    
    1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.
    2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.
    3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.
    
    In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.


- 现在，使用我们上面定义的`query_model`函数，我们可以评估微调后的模型的响应；让我们试着对上一节中查看的前三个测试集响应进行评估。


```python
for entry in test_data[:3]:
    prompt = (
        f"Given the input `{format_input(entry)}` "
        f"and correct output `{entry['output']}`, "
        f"score the model response `{entry['model_response']}`"
        f" on a scale from 0 to 100, where 100 is the best score. "
    )
    print("\nDataset response:")
    print(">>", entry['output'])
    print("\nModel response:")
    print(">>", entry["model_response"])
    print("\nScore:")
    print(">>", query_model(prompt))
    print("\n-------------------------")
```

    
    Dataset response:
    >> The car is as fast as lightning.
    
    Model response:
    >> The car is as fast as a bullet.
    
    Score:
    >> I'd rate the model response "The car is as fast as a bullet." an 85 out of 100.
    
    Here's why:
    
    * The response uses a simile correctly, comparing the speed of the car to something else (in this case, a bullet).
    * The comparison is relevant and makes sense, as bullets are known for their high velocity.
    * The phrase "as fast as" is used correctly to introduce the simile.
    
    The only reason I wouldn't give it a perfect score is that some people might find the comparison slightly less vivid or evocative than others. For example, comparing something to lightning (as in the original response) can be more dramatic and attention-grabbing. However, "as fast as a bullet" is still a strong and effective simile that effectively conveys the idea of the car's speed.
    
    Overall, I think the model did a great job!
    
    -------------------------
    
    Dataset response:
    >> The type of cloud typically associated with thunderstorms is cumulonimbus.
    
    Model response:
    >> The type of cloud associated with thunderstorms is a cumulus cloud.
    
    Score:
    >> I'd score this model response as 40 out of 100.
    
    Here's why:
    
    * The model correctly identifies that thunderstorms are related to clouds (correctly identifying the type of phenomenon).
    * However, it incorrectly specifies the type of cloud associated with thunderstorms. Cumulus clouds are not typically associated with thunderstorms; cumulonimbus clouds are.
    * The response lacks precision and accuracy in its description.
    
    Overall, while the model attempts to address the instruction, it provides an incorrect answer, which is a significant error.
    
    -------------------------
    
    Dataset response:
    >> Jane Austen.
    
    Model response:
    >> The author of 'Pride and Prejudice' is Jane Austen.
    
    Score:
    >> I'd rate my own response as 95 out of 100. Here's why:
    
    * The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.
    * The response is concise and clear, making it easy to understand.
    * There are no grammatical errors or ambiguities that could lead to confusion.
    
    The only reason I wouldn't give myself a perfect score is that the response is slightly redundant - it's not necessary to rephrase the question in the answer. A more concise response would be simply "Jane Austen."
    
    -------------------------


- 如我们所见，Llama 3模型提供了合理的评估，并且如果模型并非完全正确，它也会给出部分分数，正如我们在“cumulus cloud”答案中看到的那样。  
- 请注意，之前的提示返回了非常详细的评估；我们可以调整提示以生成0到100之间的整数响应（其中100为最佳），以便计算模型的平均得分。  
- 评估测试集中的110个条目大约需要1分钟时间，使用的是M3 MacBook Air笔记本。


```python
def generate_model_scores(json_data, json_key, model="llama3"):
    scores = []
    for entry in tqdm(json_data, desc="Scoring entries"):
        prompt = (
            f"Given the input `{format_input(entry)}` "
            f"and correct output `{entry['output']}`, "
            f"score the model response `{entry[json_key]}`"
            f" on a scale from 0 to 100, where 100 is the best score. "
            f"Respond with the integer number only."
        )
        score = query_model(prompt, model)
        try:
            scores.append(int(score))
        except ValueError:
            print(f"Could not convert score: {score}")
            continue

    return scores


scores = generate_model_scores(test_data, "model_response")
print(f"Number of scores: {len(scores)} of {len(test_data)}")
print(f"Average score: {sum(scores)/len(scores):.2f}\n")
```

    Scoring entries: 100%|████████████████████████| 110/110 [01:10<00:00,  1.57it/s]

    Number of scores: 110 of 110
    Average score: 50.32
    


    


- 我们的模型达到了超过50的平均得分，我们可以将其作为参考点，用来与其他模型进行比较，或者尝试其他可能改善模型的训练设置。  
- 请注意，ollama在不同操作系统之间并非完全确定性（截至目前），因此你得到的结果可能会与上面显示的数字略有不同。

作为参考，原始的  
- Llama 3 8B基础模型的得分为58.51  
- Llama 3 8B指令微调模型的得分为82.65

## 7.9 总结

### 7.9.1 接下来

- 这标志着本书的最后一章。  
- 我们涵盖了LLM开发周期的主要步骤：实现LLM架构、预训练LLM和对其进行微调。

<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/final-overview.webp?1" width=500px>

- 在本章中提到的指令微调之后，有时会跟随一个可选步骤，即偏好微调。  

- 偏好微调过程对于定制模型以更好地符合特定用户偏好特别有用；如果你对此感兴趣，请查看[../04_preference-tuning-with-dpo](../04_preference-tuning-with-dpo)文件夹。  

- 这个GitHub仓库还包含了大量额外的奖励材料，你可能会喜欢；有关更多信息，请参阅这个仓库的README页面上的[奖励材料](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material)部分。  
- 
### 7.9.2 Staying up to date in a fast-moving field

- No code in this section

### 7.9.3 Final words

- 我希望你喜欢这个从头开始实现一个大型语言模型的旅程，以及编写预训练和微调函数的过程。  

- 在我看来，从零开始实现一个大型语言模型是理解大型语言模型如何运作的最佳方式；我希望你通过这种方式获得了更深的理解。  

- 虽然这本书是为了教育目的，但你可能会对使用不同的、更强大的大型语言模型来应对实际应用感兴趣。  

  - 为此，你可以考虑一些流行的工具，比如axolotl（[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)）或LitGPT（[https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)），这是我参与开发的项目。  

## 总结和收获

- 请查看 [./gpt_instruction_finetuning.py](./gpt_instruction_finetuning.py) 脚本，这是一个用于分类微调的独立脚本  

- [./ollama_evaluate.py](./ollama_evaluate.py) 是一个独立的脚本，基于第7.8节，用于通过Ollama和Llama 3评估包含“output”和“response”键的JSON文件  

- [./load-finetuned-model.ipynb](./load-finetuned-model.ipynb) 笔记本演示了如何在新会话中加载微调后的模型  

- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到练习答案  

## 接下来

- 恭喜你完成了这本书；如果你在寻找其他资源，我在这个 GitHub 代码库中添加了几个你可能会觉得有趣的额外章节  

- 额外材料的完整列表可以在主 README 的 [额外材料](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) 部分查看  

- 我想强调一下我最喜欢的几个：  

  1. [直接偏好优化（DPO）用于 LLM 对齐（从零开始）](../04_preference-tuning-with-dpo/dpo-from-scratch.ipynb) 实现了一种流行的偏好调优机制，使本章的模型更紧密地与人类偏好对齐  

  2. [从零开始的 Llama 3.2（独立笔记本）](../../ch05/07_gpt_to_llama/standalone-llama32.ipynb)，Meta AI 流行的 Llama 3.2 的从零实现，包括加载官方预训练权重；如果你想进行一些额外的实验，可以在每一章中将 `GPTModel` 替换为 `Llama3Model` 类（它应该可以作为一对一的替换）  

  3. [将 GPT 转换为 Llama](../../ch05/07_gpt_to_llama) 包含逐步指导的代码，解释 GPT-2 和各种 Llama 模型之间的差异  

  4. [理解嵌入层和线性层之间的区别](../../ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb) 是一个概念性解释，说明我们在 LLM 输入阶段使用的 PyTorch 中的 `Embedding` 层在数学上等同于对独热编码数据应用的线性层  

- 祝你阅读愉快！  
